{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:05:02.940759Z",
     "start_time": "2024-07-16T20:05:02.926735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ],
   "id": "f2450ca16759c7de",
   "outputs": [],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:05:03.131741Z",
     "start_time": "2024-07-16T20:05:03.102740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "feb2624e8ab69803",
   "outputs": [],
   "execution_count": 206
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:05:03.163741Z",
     "start_time": "2024-07-16T20:05:03.154739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "89b72c1f1c183626",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Getting the Data\n",
    "* To begin, we'll establish the directory path to our dataset.\n",
    "\n",
    "* The dataset we're using is sourced from Kaggle and can be accessed [here](https://www.kaggle.com/datasets/michaelarman/poemsdataset).\n",
    "\n",
    "* This collection includes poems by various authors. Our goal is to create a program that can generate a poem based on an initial line provided by the user."
   ],
   "id": "5b97e7539a08be21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:35.563172Z",
     "start_time": "2024-07-16T20:05:03.212737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reading the data from file\n",
    "path = 'Data/forms'\n",
    "# opening every folder within the directory and reading each of the text files within the folder and saving it in a list\n",
    "data = []\n",
    "for folder in os.listdir(path):\n",
    "    for file in os.listdir(os.path.join(path, folder)):\n",
    "        with open(os.path.join(path, folder, file), 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exploratory Data Analysis\n",
    "* We'll begin by examining the first five entries in our dataset.\n",
    "* Next, we'll determine the length of the dataset.\n",
    "* Finally, we'll calculate the number of unique words in the dataset.\n",
    "* We'll also determine the number of unique characters in the dataset."
   ],
   "id": "6d0e36abd99ca27d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:35.579149Z",
     "start_time": "2024-07-16T20:06:35.567576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking the first 5 data\n",
    "for i in range(5):\n",
    "    print(\"=\"*50)\n",
    "    print(data[i])"
   ],
   "id": "686199096169db4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "2 ABC of H.k. and China revised vision.\n",
      "Barrels tears are wines and salts.\n",
      "With a whisk on goody tails!\n",
      "Wiggle maces to fix the heads.\n",
      "Heads in jack on boxes are ceased.\n",
      "Cry to paranoid truly bosses.\n",
      "Bosses are jokers take your boys.\n",
      "Studs are bogs with fire apples.\n",
      "True predicates worth cases.’\n",
      "Descents wash in badly bands.\n",
      "Wholly sales are smart with cats.\n",
      "Who got tenth honors in China?\n",
      "Homage grand to play and plays!\n",
      "Trim the times of hearts then cry.\n",
      "Tanks in steels but voice wail.\n",
      "Bossy dragged by tails that whisked.\n",
      "Go very timid and love the wise.\n",
      "Hands are lent but laws are ends.\n",
      "Cases on courts are borrowed lands.\n",
      "Length long with treads to retch!\n",
      "Straps on times and watch here.\n",
      "Arrays tanks but all are men.\n",
      "Cross all suctions steal the ends.\n",
      "Cave on minds are cages on objects.\n",
      "Rouser rockets powers holes.\n",
      "Confine curses to stop our wounds.\n",
      "Whirl your bodies and jump on grounds.\n",
      "Crouch of soldiers after kicks with flings.\n",
      "Block one leg and hit the middle.\n",
      "Cauchy3 know the tricks to kill.\n",
      "Threaten weak oppressed ill.\n",
      "Surpass scores are bad in honors.\n",
      "Wash to think that build the homes.\n",
      "Angel sins but cauchy3 has funs.\n",
      "Make ones tools when hats are found.\n",
      "Worlds are drawers on bottom noses.\n",
      "Singular ugly piece is rose.\n",
      "Wily mores are teeth of sharks.\n",
      "Saw with tooth is laws in arts.\n",
      "Artful men power with grids.\n",
      "Bodies stamped and wills are ridden.\n",
      "Sign in forth with battles conquered.\n",
      "Triumphs on candles whip the stands.\n",
      "Soups are soaps and faiths not come.\n",
      "We are meats in balls and rice to constants.\n",
      "---Cheung Shun Sang=Cauchy3---\n",
      "==================================================\n",
      "Apparently life without love, is no life at all.\n",
      "Become an advocate of true love.\n",
      "Control your innermost emotions.\n",
      "Decide your worth, fulfill your desires.\n",
      "Written:  Oct.16/06\n",
      "==================================================\n",
      "A abc angles on angels flaws (poem)\n",
      "Mix with means and assets\n",
      "Passive and active to abase the men\n",
      "Heels and knees rent for renown\n",
      "Breeds the breads to be demanded\n",
      "Assert to assist by blocks of boards\n",
      "Necks and necks to bad affairs\n",
      "Moneys pay for debts in doubles\n",
      "Laws are raw to protect with doubts\n",
      "Shams are shames to have costs\n",
      "Shams on sheets incubated courses\n",
      "Lack of actions to freedoms and justices\n",
      "Nouns are in ounces and loves are in pounds\n",
      "Control the concepts of wielding guns\n",
      "Control the concepts of smith for guns\n",
      "Put the concepts of tyrants on hell\n",
      "To hell real tyrants have helps\n",
      "Hounds and eagles should focus on hunting\n",
      "Expect the evils to be saints on junctions\n",
      "Axes and awls are lying below\n",
      "Awnings are answers with piss and pee\n",
      "Peek on peaks of powers to be laws\n",
      "Whales on whips are wheat to sharks\n",
      "Weeps when sweep in clocks of Gods\n",
      "Corrections of cotillions like hoofs\n",
      "Zeroes in zeal for facts and morals\n",
      "Morales are guns that march and move\n",
      "Laws are lost and twist a lot\n",
      "Terms in targets have not effects in long\n",
      "Stay in dark or run from dart.\n",
      "Stray to cycles of cruel ranges\n",
      "Laws as can be rights are none\n",
      "Table lands are tables with spoons\n",
      "Monks have manners to list on tables\n",
      "Jack apples are appetites on tabs\n",
      "Whales on whips are wheat to sharks\n",
      "Baby fishes are preys in bays\n",
      "To be back in the passes to wash by bloods\n",
      "In themes of theory our rights are loud\n",
      "Next days are dated with truths or false\n",
      "Flaws in angles of angels are small\n",
      "Wings when wink to rains give no shelters\n",
      "Bloods are buses to carry to hell\n",
      "Wash their shames but leave our rights\n",
      "A network of flatterers is in wires\n",
      "--Political poem by Cheung Shun Sang=Cauchy3--\n",
      "==================================================\n",
      "A abc Brazil dance (poem)\n",
      "Jack of crack in pots of crack\n",
      "Sambas have bugs on figs that take\n",
      "My heart has unseen bugs in acting\n",
      "Ghosts haunt in cavern with heal\n",
      "Empty stream in holes to steal\n",
      "Barrow has one wheel for near\n",
      "Wheel and deal are those on top\n",
      "Tares are on suits but meadows are women\n",
      "Dilate the prizes of prides on top\n",
      "We are all cogs to tone\n",
      "Harps be fine and fingers be good\n",
      "Jacks play with their fingers on goods\n",
      "The sun on Asia sky may be red\n",
      "Give the logistics are logics to pets\n",
      "Patterns of Gods are king s bodies.\n",
      "Launch the papers on lunches to bowls\n",
      "That faces of poets help to smiles\n",
      "Admire the distances from hade to a mile\n",
      "Pleasures of leaders are earned by rows\n",
      "Would there be sweats on brows when bow\n",
      "Pietisms on parts are wholesome in vices.\n",
      "Wise in acre but bully to eyes\n",
      "Rusts are here yellows are pearls\n",
      "Dittos on powers but flaw to react\n",
      "Crooks roll your minds with hopes\n",
      "Cogs in times lead us homes\n",
      "Straps on times watch ones feet\n",
      "Even are evil while equal are eerie\n",
      "Only our planet with one sun\n",
      "Falcons are slow down on bushes\n",
      "Salty weathers salute to soldiers\n",
      "Sambas of fortunes have just to sold\n",
      "Passes on stages are passages to fires\n",
      "Books on accounts are moneys to fine\n",
      "Sambas of fortunes have just to sold\n",
      "Sambas of fortunes have just to sold\n",
      "Vectors pf suns are heats and flames\n",
      "Bind to lines on sky when claimed\n",
      "Plash and slash on lakes when free.\n",
      "Brazil should breach on tank s wheels\n",
      "Stand off and away from tanks\n",
      "Red suns have white heats\n",
      "Friends for wines and for wench\n",
      "Stand away from tanks ends\n",
      "--Political poem by Cheung Shun Sang=Cauchy3--\n",
      "==================================================\n",
      "ABC... I can't go on\n",
      "123... what's the next one?\n",
      "My mind is scrambled\n",
      "My heart is confused\n",
      "You're holding on to me.\n",
      "What do I do?\n",
      "What's the next letter in my name?\n",
      "What's the next emotion I'll exclaim?\n",
      "A sound is part of a word and a word, a meaning\n",
      "Just like you want to be part of me\n",
      "And you are, though you can't see\n",
      "Without you, darling, there is not me\n",
      "I don't know which way is up\n",
      "And without you, I don't feel much\n",
      "You say 'I'm holding on, and I'm not letting go'\n",
      "You said the same things to her, you know\n",
      "Then you dropped her, just like that\n",
      "And you'll leave me at the dropp of a hat\n",
      "I know you say, it's not the same\n",
      "But darling, i don't feel that way\n",
      "I don't share your infatuation\n",
      "You need a reevaluation\n",
      "Of your emotions, stop for a minute. Think.\n",
      "You won't get drunk if ya don't take a drink\n",
      "Ya get me? understand, please\n",
      "Please stop idolizing me!\n",
      "Just because I'm saying no\n",
      "Doesn't mean i'm playing hard-to-get\n",
      "Sometimes, no means no, hun\n",
      "That's something not to forget\n",
      "ABC... I can't go on\n",
      "123... what's the next one?\n",
      "My mind is scrambled\n",
      "My heart is confused\n",
      "You're holding on to me.\n",
      "What do I do?\n",
      "What's the next letter in my name?\n",
      "What's the next emotion I'll exclaim?\n",
      "I'm being honest with you at best\n",
      "Breaking the eroding heart inside my chest\n",
      "What's the next letter in my name?\n",
      "What's the next emotion I'll exclaim?\n",
      "A sound is part of a word and a word, a meaning\n",
      "Just like you want to be part of me\n"
     ]
    }
   ],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:35.610285Z",
     "start_time": "2024-07-16T20:06:35.584264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking the length of the data\n",
    "print(len(data))"
   ],
   "id": "c1b3310870cc3caa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6322\n"
     ]
    }
   ],
   "execution_count": 210
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`This indicates the number of unique words in the dataset on which our model will be trained.`",
   "id": "30340190bbce2f19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.202782Z",
     "start_time": "2024-07-16T20:06:35.615821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# checking the number of unique words in the data\n",
    "words = []\n",
    "for i in data:\n",
    "    words.extend(i.split())\n",
    "print(len(set(words)))"
   ],
   "id": "cc06075c2cbef42e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138244\n"
     ]
    }
   ],
   "execution_count": 211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.359982Z",
     "start_time": "2024-07-16T20:06:36.205981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char = sorted(list(set(''.join(data))))\n",
    "\n",
    "print(\"Number of unique characters in the data: \", len(char))\n",
    "print(\"Unique characters in the data: \", char)\n",
    "\n",
    "vocabulary_size = len(char)"
   ],
   "id": "86dfd7fe8cda19d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters in the data:  1054\n",
      "Unique characters in the data:  ['\\x07', '\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\xa0', '¡', '£', '¤', '¦', '§', '©', '«', '\\xad', '®', '°', '²', '´', '·', 'º', '»', '½', '¿', 'À', 'Á', 'Ã', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ñ', 'Ó', 'Ô', 'Ö', 'Ø', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'þ', 'ā', 'ă', 'ć', 'č', 'Đ', 'đ', 'ĩ', 'ī', 'Œ', 'œ', 'ś', 'Ş', 'ş', 'Š', 'š', 'ţ', 'Ž', 'ž', 'ơ', 'ư', '˝', '̀', '́', '̃', '̄', '̉', '̣', 'Ά', '·', 'Γ', 'Ε', 'ά', 'έ', 'ή', 'ί', 'α', 'β', 'γ', 'δ', 'ε', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ω', 'ό', 'ύ', 'Х', 'с', '،', 'آ', 'ئ', 'ا', 'ب', 'ت', 'ج', 'ح', 'خ', 'د', 'ر', 'ز', 'س', 'ش', 'ص', 'ط', 'ع', 'غ', 'ف', 'ك', 'ل', 'م', 'ن', 'و', 'ِ', 'ٹ', 'پ', 'چ', 'ڑ', 'ک', 'گ', 'ں', 'ھ', 'ہ', 'ی', 'ے', '۔', 'ँ', 'ं', 'अ', 'आ', 'इ', 'ई', 'उ', 'ए', 'ऐ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'च', 'ज', 'झ', 'ट', 'ठ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'े', 'ै', 'ो', 'ौ', '्', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', '।', 'ঁ', 'ং', 'ঃ', 'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঋ', 'এ', 'ঐ', 'ও', 'ঔ', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঝ', 'ঞ', 'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ', 'স', 'হ', '়', 'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', 'ৌ', '্', 'ৎ', 'ড়', 'ঢ়', 'য়', '০', '১', '২', '৩', '৪', '৫', '৬', '৭', '৮', '৯', 'ạ', 'ả', 'ầ', 'ậ', 'ắ', 'ằ', 'ẳ', 'ặ', 'ẹ', 'ẻ', 'ế', 'ề', 'ể', 'ệ', 'ỉ', 'ị', 'ọ', 'ỏ', 'ố', 'ồ', 'ộ', 'ớ', 'ờ', 'ở', 'ợ', 'ứ', '\\u2009', '\\u200b', '\\u200c', '‑', '–', '—', '‘', '’', '‚', '“', '”', '„', '†', '•', '…', '−', '─', '│', '▌', '▐', '▲', '◆', '●', '★', '☆', '♥', '。', '《', '》', '一', '七', '三', '上', '下', '不', '与', '且', '世', '东', '丝', '两', '丧', '个', '中', '丰', '丹', '为', '主', '丽', '之', '乐', '乘', '九', '了', '争', '于', '云', '五', '亚', '享', '京', '亮', '亲', '人', '今', '他', '仙', '代', '们', '伊', '优', '会', '传', '伤', '伴', '但', '位', '住', '体', '何', '你', '使', '侄', '俏', '信', '倾', '健', '像', '儿', '允', '充', '先', '光', '入', '全', '八', '六', '共', '兴', '兹', '内', '写', '冷', '净', '凝', '出', '分', '初', '利', '别', '到', '刻', '加', '动', '北', '医', '十', '午', '华', '印', '卷', '去', '县', '友', '古', '句', '另', '只', '可', '叶', '号', '叹', '合', '吉', '名', '后', '君', '吞', '吟', '吼', '呼', '和', '咽', '哀', '响', '哦', '哭', '哽', '唐', '唤', '唱', '啊', '善', '喜', '喷', '嘈', '嘹', '四', '因', '团', '园', '国', '圣', '在', '地', '城', '堂', '塔', '境', '士', '声', '处', '复', '夕', '多', '夜', '大', '天', '太', '失', '夹', '奉', '女', '她', '好', '如', '妈', '姚', '娅', '娟', '娥', '娩', '婵', '子', '字', '孙', '孟', '孤', '孩', '宅', '安', '官', '宵', '家', '对', '寺', '寿', '少', '尔', '就', '层', '山', '岁', '岸', '崇', '已', '市', '布', '希', '带', '席', '平', '年', '幽', '广', '府', '座', '康', '建', '开', '弄', '弦', '弹', '归', '当', '得', '微', '心', '忆', '志', '忧', '怒', '思', '总', '恐', '恒', '恢', '恨', '恩', '恳', '恸', '悠', '悦', '悲', '悴', '情', '惊', '惜', '惧', '想', '愁', '愉', '意', '感', '愿', '慈', '慢', '憔', '我', '戛', '所', '才', '扭', '承', '把', '抓', '拉', '拜', '拨', '挽', '摸', '撼', '救', '教', '散', '敬', '整', '新', '方', '施', '无', '日', '旦', '早', '时', '旷', '明', '星', '春', '是', '晚', '晨', '景', '暖', '暮', '暴', '曲', '更', '曼', '最', '會', '月', '有', '朗', '望', '朝', '木', '朵', '机', '朽', '杂', '李', '束', '条', '来', '构', '林', '枝', '染', '查', '树', '栖', '桃', '梦', '梨', '检', '次', '欢', '欲', '歌', '止', '正', '此', '残', '殡', '母', '每', '比', '氏', '气', '水', '永', '求', '汉', '池', '污', '汩', '汹', '沉', '沐', '沒', '沙', '沦', '河', '泉', '泊', '波', '泣', '注', '泪', '泻', '泽', '洁', '洋', '洒', '洞', '津', '活', '浓', '浪', '浴', '海', '涌', '涨', '液', '淌', '淙', '深', '淹', '清', '渔', '温', '渴', '湃', '湖', '源', '满', '演', '漫', '潇', '潸', '潺', '澎', '火', '灯', '灶', '炉', '点', '炽', '烟', '烧', '热', '焦', '然', '煌', '燃', '爱', '牡', '狂', '独', '献', '玉', '王', '现', '珀', '班', '球', '琴', '琼', '瓶', '生', '用', '由', '画', '界', '痛', '痴', '白', '百', '的', '皆', '盘', '盛', '目', '直', '相', '省', '眉', '看', '眸', '眼', '着', '知', '碎', '磬', '祈', '祝', '神', '祥', '祭', '祷', '福', '离', '私', '秃', '秋', '种', '稀', '穴', '空', '穿', '窗', '竖', '童', '端', '竹', '笑', '简', '籁', '类', '紧', '紫', '繁', '红', '纯', '纽', '线', '绊', '经', '结', '绝', '统', '绪', '绵', '缓', '缘', '缠', '罐', '罗', '羁', '美', '翩', '翻', '翼', '耀', '而', '耳', '联', '聚', '肺', '脉', '臣', '自', '至', '致', '舞', '舟', '色', '节', '芒', '花', '英', '荣', '荷', '莱', '莲', '菊', '萎', '萝', '落', '蒸', '蓝', '藏', '虑', '虚', '虽', '血', '行', '裴', '裸', '襄', '西', '要', '親', '认', '议', '许', '译', '试', '诗', '诞', '调', '谢', '费', '赎', '赞', '赫', '走', '赶', '起', '跟', '跪', '路', '跹', '轻', '载', '辈', '辉', '边', '达', '近', '这', '远', '迟', '追', '通', '逝', '過', '遥', '那', '部', '都', '酒', '酬', '酸', '醒', '里', '野', '金', '钟', '银', '铺', '锁', '错', '镜', '長', '长', '门', '问', '间', '闻', '阳', '阴', '阵', '陆', '陽', '隨', '雁', '雅', '雪', '雾', '震', '霜', '霭', '露', '青', '静', '音', '韵', '颂', '频', '题', '颜', '颤', '风', '飘', '飞', '首', '馨', '马', '高', '龙', '\\uf04a', '\\uf076', '\\uf0d8', '！', '（', '）', '，', '：', '；', '￼']\n"
     ]
    }
   ],
   "execution_count": 212
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Observations\n",
    "* The dataset contains 6322 poems.\n",
    "* The number of unique words in the dataset is 1,38,244.\n",
    "* The number of unique characters in the dataset is 1054."
   ],
   "id": "c555707d87dddf6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.594557Z",
     "start_time": "2024-07-16T20:06:36.363239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# writing the unqiue words and characters to seperate files\n",
    "with open('Data/unique_words.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(set(words)))\n",
    "with open('Data/unique_characters.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(char))"
   ],
   "id": "c718ad04b01fdb",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating the Encoder and Decoder\n",
    "* We'll now create the encoder and decoder for our model.\n",
    "* The encoder will convert the input text into a tensor.\n",
    "* The decoder will convert the tensor back into text."
   ],
   "id": "172ef9e656b4012a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.610129Z",
     "start_time": "2024-07-16T20:06:36.596925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encoder\n",
    "# using a basic dictionary to convert the characters to integers\n",
    "char_to_int = {char: i for i, char in enumerate(char)}"
   ],
   "id": "69b0763bee0aa201",
   "outputs": [],
   "execution_count": 214
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.640291Z",
     "start_time": "2024-07-16T20:06:36.614215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Decoder\n",
    "# using a basic dictionary to convert the integers back to characters\n",
    "int_to_char = {i: char for i, char in enumerate(char)}"
   ],
   "id": "c001f7ce81255e4a",
   "outputs": [],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.656350Z",
     "start_time": "2024-07-16T20:06:36.642291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating a function to convert the text to tensor\n",
    "def text_to_tensor(text, char_to_int):\n",
    "    tensor = torch.tensor([char_to_int[char] for char in text], dtype=torch.long).to(device)\n",
    "    return tensor.tolist()"
   ],
   "id": "ed0108c3165f5a16",
   "outputs": [],
   "execution_count": 216
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.704100Z",
     "start_time": "2024-07-16T20:06:36.685962Z"
    }
   },
   "cell_type": "code",
   "source": "print(text_to_tensor(data[0], char_to_int))",
   "id": "3259eb512e0918d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 3, 36, 37, 38, 3, 82, 73, 3, 43, 17, 78, 17, 3, 68, 81, 71, 3, 38, 75, 76, 81, 68, 3, 85, 72, 89, 76, 86, 72, 71, 3, 89, 76, 86, 76, 82, 81, 17, 2, 37, 68, 85, 85, 72, 79, 86, 3, 87, 72, 68, 85, 86, 3, 68, 85, 72, 3, 90, 76, 81, 72, 86, 3, 68, 81, 71, 3, 86, 68, 79, 87, 86, 17, 2, 58, 76, 87, 75, 3, 68, 3, 90, 75, 76, 86, 78, 3, 82, 81, 3, 74, 82, 82, 71, 92, 3, 87, 68, 76, 79, 86, 4, 2, 58, 76, 74, 74, 79, 72, 3, 80, 68, 70, 72, 86, 3, 87, 82, 3, 73, 76, 91, 3, 87, 75, 72, 3, 75, 72, 68, 71, 86, 17, 2, 43, 72, 68, 71, 86, 3, 76, 81, 3, 77, 68, 70, 78, 3, 82, 81, 3, 69, 82, 91, 72, 86, 3, 68, 85, 72, 3, 70, 72, 68, 86, 72, 71, 17, 2, 38, 85, 92, 3, 87, 82, 3, 83, 68, 85, 68, 81, 82, 76, 71, 3, 87, 85, 88, 79, 92, 3, 69, 82, 86, 86, 72, 86, 17, 2, 37, 82, 86, 86, 72, 86, 3, 68, 85, 72, 3, 77, 82, 78, 72, 85, 86, 3, 87, 68, 78, 72, 3, 92, 82, 88, 85, 3, 69, 82, 92, 86, 17, 2, 54, 87, 88, 71, 86, 3, 68, 85, 72, 3, 69, 82, 74, 86, 3, 90, 76, 87, 75, 3, 73, 76, 85, 72, 3, 68, 83, 83, 79, 72, 86, 17, 2, 55, 85, 88, 72, 3, 83, 85, 72, 71, 76, 70, 68, 87, 72, 86, 3, 90, 82, 85, 87, 75, 3, 70, 68, 86, 72, 86, 17, 419, 2, 39, 72, 86, 70, 72, 81, 87, 86, 3, 90, 68, 86, 75, 3, 76, 81, 3, 69, 68, 71, 79, 92, 3, 69, 68, 81, 71, 86, 17, 2, 58, 75, 82, 79, 79, 92, 3, 86, 68, 79, 72, 86, 3, 68, 85, 72, 3, 86, 80, 68, 85, 87, 3, 90, 76, 87, 75, 3, 70, 68, 87, 86, 17, 2, 58, 75, 82, 3, 74, 82, 87, 3, 87, 72, 81, 87, 75, 3, 75, 82, 81, 82, 85, 86, 3, 76, 81, 3, 38, 75, 76, 81, 68, 34, 2, 43, 82, 80, 68, 74, 72, 3, 74, 85, 68, 81, 71, 3, 87, 82, 3, 83, 79, 68, 92, 3, 68, 81, 71, 3, 83, 79, 68, 92, 86, 4, 2, 55, 85, 76, 80, 3, 87, 75, 72, 3, 87, 76, 80, 72, 86, 3, 82, 73, 3, 75, 72, 68, 85, 87, 86, 3, 87, 75, 72, 81, 3, 70, 85, 92, 17, 2, 55, 68, 81, 78, 86, 3, 76, 81, 3, 86, 87, 72, 72, 79, 86, 3, 69, 88, 87, 3, 89, 82, 76, 70, 72, 3, 90, 68, 76, 79, 17, 2, 37, 82, 86, 86, 92, 3, 71, 85, 68, 74, 74, 72, 71, 3, 69, 92, 3, 87, 68, 76, 79, 86, 3, 87, 75, 68, 87, 3, 90, 75, 76, 86, 78, 72, 71, 17, 2, 42, 82, 3, 89, 72, 85, 92, 3, 87, 76, 80, 76, 71, 3, 68, 81, 71, 3, 79, 82, 89, 72, 3, 87, 75, 72, 3, 90, 76, 86, 72, 17, 2, 43, 68, 81, 71, 86, 3, 68, 85, 72, 3, 79, 72, 81, 87, 3, 69, 88, 87, 3, 79, 68, 90, 86, 3, 68, 85, 72, 3, 72, 81, 71, 86, 17, 2, 38, 68, 86, 72, 86, 3, 82, 81, 3, 70, 82, 88, 85, 87, 86, 3, 68, 85, 72, 3, 69, 82, 85, 85, 82, 90, 72, 71, 3, 79, 68, 81, 71, 86, 17, 2, 47, 72, 81, 74, 87, 75, 3, 79, 82, 81, 74, 3, 90, 76, 87, 75, 3, 87, 85, 72, 68, 71, 86, 3, 87, 82, 3, 85, 72, 87, 70, 75, 4, 2, 54, 87, 85, 68, 83, 86, 3, 82, 81, 3, 87, 76, 80, 72, 86, 3, 68, 81, 71, 3, 90, 68, 87, 70, 75, 3, 75, 72, 85, 72, 17, 2, 36, 85, 85, 68, 92, 86, 3, 87, 68, 81, 78, 86, 3, 69, 88, 87, 3, 68, 79, 79, 3, 68, 85, 72, 3, 80, 72, 81, 17, 2, 38, 85, 82, 86, 86, 3, 68, 79, 79, 3, 86, 88, 70, 87, 76, 82, 81, 86, 3, 86, 87, 72, 68, 79, 3, 87, 75, 72, 3, 72, 81, 71, 86, 17, 2, 38, 68, 89, 72, 3, 82, 81, 3, 80, 76, 81, 71, 86, 3, 68, 85, 72, 3, 70, 68, 74, 72, 86, 3, 82, 81, 3, 82, 69, 77, 72, 70, 87, 86, 17, 2, 53, 82, 88, 86, 72, 85, 3, 85, 82, 70, 78, 72, 87, 86, 3, 83, 82, 90, 72, 85, 86, 3, 75, 82, 79, 72, 86, 17, 2, 38, 82, 81, 73, 76, 81, 72, 3, 70, 88, 85, 86, 72, 86, 3, 87, 82, 3, 86, 87, 82, 83, 3, 82, 88, 85, 3, 90, 82, 88, 81, 71, 86, 17, 2, 58, 75, 76, 85, 79, 3, 92, 82, 88, 85, 3, 69, 82, 71, 76, 72, 86, 3, 68, 81, 71, 3, 77, 88, 80, 83, 3, 82, 81, 3, 74, 85, 82, 88, 81, 71, 86, 17, 2, 38, 85, 82, 88, 70, 75, 3, 82, 73, 3, 86, 82, 79, 71, 76, 72, 85, 86, 3, 68, 73, 87, 72, 85, 3, 78, 76, 70, 78, 86, 3, 90, 76, 87, 75, 3, 73, 79, 76, 81, 74, 86, 17, 2, 37, 79, 82, 70, 78, 3, 82, 81, 72, 3, 79, 72, 74, 3, 68, 81, 71, 3, 75, 76, 87, 3, 87, 75, 72, 3, 80, 76, 71, 71, 79, 72, 17, 2, 38, 68, 88, 70, 75, 92, 22, 3, 78, 81, 82, 90, 3, 87, 75, 72, 3, 87, 85, 76, 70, 78, 86, 3, 87, 82, 3, 78, 76, 79, 79, 17, 2, 55, 75, 85, 72, 68, 87, 72, 81, 3, 90, 72, 68, 78, 3, 82, 83, 83, 85, 72, 86, 86, 72, 71, 3, 76, 79, 79, 17, 2, 54, 88, 85, 83, 68, 86, 86, 3, 86, 70, 82, 85, 72, 86, 3, 68, 85, 72, 3, 69, 68, 71, 3, 76, 81, 3, 75, 82, 81, 82, 85, 86, 17, 2, 58, 68, 86, 75, 3, 87, 82, 3, 87, 75, 76, 81, 78, 3, 87, 75, 68, 87, 3, 69, 88, 76, 79, 71, 3, 87, 75, 72, 3, 75, 82, 80, 72, 86, 17, 2, 36, 81, 74, 72, 79, 3, 86, 76, 81, 86, 3, 69, 88, 87, 3, 70, 68, 88, 70, 75, 92, 22, 3, 75, 68, 86, 3, 73, 88, 81, 86, 17, 2, 48, 68, 78, 72, 3, 82, 81, 72, 86, 3, 87, 82, 82, 79, 86, 3, 90, 75, 72, 81, 3, 75, 68, 87, 86, 3, 68, 85, 72, 3, 73, 82, 88, 81, 71, 17, 2, 58, 82, 85, 79, 71, 86, 3, 68, 85, 72, 3, 71, 85, 68, 90, 72, 85, 86, 3, 82, 81, 3, 69, 82, 87, 87, 82, 80, 3, 81, 82, 86, 72, 86, 17, 2, 54, 76, 81, 74, 88, 79, 68, 85, 3, 88, 74, 79, 92, 3, 83, 76, 72, 70, 72, 3, 76, 86, 3, 85, 82, 86, 72, 17, 2, 58, 76, 79, 92, 3, 80, 82, 85, 72, 86, 3, 68, 85, 72, 3, 87, 72, 72, 87, 75, 3, 82, 73, 3, 86, 75, 68, 85, 78, 86, 17, 2, 54, 68, 90, 3, 90, 76, 87, 75, 3, 87, 82, 82, 87, 75, 3, 76, 86, 3, 79, 68, 90, 86, 3, 76, 81, 3, 68, 85, 87, 86, 17, 2, 36, 85, 87, 73, 88, 79, 3, 80, 72, 81, 3, 83, 82, 90, 72, 85, 3, 90, 76, 87, 75, 3, 74, 85, 76, 71, 86, 17, 2, 37, 82, 71, 76, 72, 86, 3, 86, 87, 68, 80, 83, 72, 71, 3, 68, 81, 71, 3, 90, 76, 79, 79, 86, 3, 68, 85, 72, 3, 85, 76, 71, 71, 72, 81, 17, 2, 54, 76, 74, 81, 3, 76, 81, 3, 73, 82, 85, 87, 75, 3, 90, 76, 87, 75, 3, 69, 68, 87, 87, 79, 72, 86, 3, 70, 82, 81, 84, 88, 72, 85, 72, 71, 17, 2, 55, 85, 76, 88, 80, 83, 75, 86, 3, 82, 81, 3, 70, 68, 81, 71, 79, 72, 86, 3, 90, 75, 76, 83, 3, 87, 75, 72, 3, 86, 87, 68, 81, 71, 86, 17, 2, 54, 82, 88, 83, 86, 3, 68, 85, 72, 3, 86, 82, 68, 83, 86, 3, 68, 81, 71, 3, 73, 68, 76, 87, 75, 86, 3, 81, 82, 87, 3, 70, 82, 80, 72, 17, 2, 58, 72, 3, 68, 85, 72, 3, 80, 72, 68, 87, 86, 3, 76, 81, 3, 69, 68, 79, 79, 86, 3, 68, 81, 71, 3, 85, 76, 70, 72, 3, 87, 82, 3, 70, 82, 81, 86, 87, 68, 81, 87, 86, 17, 2, 16, 16, 16, 38, 75, 72, 88, 81, 74, 3, 54, 75, 88, 81, 3, 54, 68, 81, 74, 32, 38, 68, 88, 70, 75, 92, 22, 16, 16, 16]\n"
     ]
    }
   ],
   "execution_count": 217
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.735196Z",
     "start_time": "2024-07-16T20:06:36.707415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating a function to convert the tensor back to text\n",
    "def tensor_to_text(tensor, int_to_char):\n",
    "    return ''.join([int_to_char[i] for i in tensor])"
   ],
   "id": "e3ddc956b24eeb3f",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.751089Z",
     "start_time": "2024-07-16T20:06:36.738242Z"
    }
   },
   "cell_type": "code",
   "source": "print(tensor_to_text(text_to_tensor(data[0], char_to_int), int_to_char))",
   "id": "94c9cd1d2700761",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ABC of H.k. and China revised vision.\n",
      "Barrels tears are wines and salts.\n",
      "With a whisk on goody tails!\n",
      "Wiggle maces to fix the heads.\n",
      "Heads in jack on boxes are ceased.\n",
      "Cry to paranoid truly bosses.\n",
      "Bosses are jokers take your boys.\n",
      "Studs are bogs with fire apples.\n",
      "True predicates worth cases.’\n",
      "Descents wash in badly bands.\n",
      "Wholly sales are smart with cats.\n",
      "Who got tenth honors in China?\n",
      "Homage grand to play and plays!\n",
      "Trim the times of hearts then cry.\n",
      "Tanks in steels but voice wail.\n",
      "Bossy dragged by tails that whisked.\n",
      "Go very timid and love the wise.\n",
      "Hands are lent but laws are ends.\n",
      "Cases on courts are borrowed lands.\n",
      "Length long with treads to retch!\n",
      "Straps on times and watch here.\n",
      "Arrays tanks but all are men.\n",
      "Cross all suctions steal the ends.\n",
      "Cave on minds are cages on objects.\n",
      "Rouser rockets powers holes.\n",
      "Confine curses to stop our wounds.\n",
      "Whirl your bodies and jump on grounds.\n",
      "Crouch of soldiers after kicks with flings.\n",
      "Block one leg and hit the middle.\n",
      "Cauchy3 know the tricks to kill.\n",
      "Threaten weak oppressed ill.\n",
      "Surpass scores are bad in honors.\n",
      "Wash to think that build the homes.\n",
      "Angel sins but cauchy3 has funs.\n",
      "Make ones tools when hats are found.\n",
      "Worlds are drawers on bottom noses.\n",
      "Singular ugly piece is rose.\n",
      "Wily mores are teeth of sharks.\n",
      "Saw with tooth is laws in arts.\n",
      "Artful men power with grids.\n",
      "Bodies stamped and wills are ridden.\n",
      "Sign in forth with battles conquered.\n",
      "Triumphs on candles whip the stands.\n",
      "Soups are soaps and faiths not come.\n",
      "We are meats in balls and rice to constants.\n",
      "---Cheung Shun Sang=Cauchy3---\n"
     ]
    }
   ],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.782124Z",
     "start_time": "2024-07-16T20:06:36.754099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# converting specific characters to tensor and back to text\n",
    "encoded_text = text_to_tensor(data[0], char_to_int)\n",
    "decoded_text = tensor_to_text(encoded_text, int_to_char)\n",
    "print(\"Original Text: \", data[0])"
   ],
   "id": "e6e352c6c80f1d65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  2 ABC of H.k. and China revised vision.\n",
      "Barrels tears are wines and salts.\n",
      "With a whisk on goody tails!\n",
      "Wiggle maces to fix the heads.\n",
      "Heads in jack on boxes are ceased.\n",
      "Cry to paranoid truly bosses.\n",
      "Bosses are jokers take your boys.\n",
      "Studs are bogs with fire apples.\n",
      "True predicates worth cases.’\n",
      "Descents wash in badly bands.\n",
      "Wholly sales are smart with cats.\n",
      "Who got tenth honors in China?\n",
      "Homage grand to play and plays!\n",
      "Trim the times of hearts then cry.\n",
      "Tanks in steels but voice wail.\n",
      "Bossy dragged by tails that whisked.\n",
      "Go very timid and love the wise.\n",
      "Hands are lent but laws are ends.\n",
      "Cases on courts are borrowed lands.\n",
      "Length long with treads to retch!\n",
      "Straps on times and watch here.\n",
      "Arrays tanks but all are men.\n",
      "Cross all suctions steal the ends.\n",
      "Cave on minds are cages on objects.\n",
      "Rouser rockets powers holes.\n",
      "Confine curses to stop our wounds.\n",
      "Whirl your bodies and jump on grounds.\n",
      "Crouch of soldiers after kicks with flings.\n",
      "Block one leg and hit the middle.\n",
      "Cauchy3 know the tricks to kill.\n",
      "Threaten weak oppressed ill.\n",
      "Surpass scores are bad in honors.\n",
      "Wash to think that build the homes.\n",
      "Angel sins but cauchy3 has funs.\n",
      "Make ones tools when hats are found.\n",
      "Worlds are drawers on bottom noses.\n",
      "Singular ugly piece is rose.\n",
      "Wily mores are teeth of sharks.\n",
      "Saw with tooth is laws in arts.\n",
      "Artful men power with grids.\n",
      "Bodies stamped and wills are ridden.\n",
      "Sign in forth with battles conquered.\n",
      "Triumphs on candles whip the stands.\n",
      "Soups are soaps and faiths not come.\n",
      "We are meats in balls and rice to constants.\n",
      "---Cheung Shun Sang=Cauchy3---\n"
     ]
    }
   ],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.797516Z",
     "start_time": "2024-07-16T20:06:36.785833Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Encoded Text: \", encoded_text)",
   "id": "f41b0cfc0ccb20b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Text:  [21, 3, 36, 37, 38, 3, 82, 73, 3, 43, 17, 78, 17, 3, 68, 81, 71, 3, 38, 75, 76, 81, 68, 3, 85, 72, 89, 76, 86, 72, 71, 3, 89, 76, 86, 76, 82, 81, 17, 2, 37, 68, 85, 85, 72, 79, 86, 3, 87, 72, 68, 85, 86, 3, 68, 85, 72, 3, 90, 76, 81, 72, 86, 3, 68, 81, 71, 3, 86, 68, 79, 87, 86, 17, 2, 58, 76, 87, 75, 3, 68, 3, 90, 75, 76, 86, 78, 3, 82, 81, 3, 74, 82, 82, 71, 92, 3, 87, 68, 76, 79, 86, 4, 2, 58, 76, 74, 74, 79, 72, 3, 80, 68, 70, 72, 86, 3, 87, 82, 3, 73, 76, 91, 3, 87, 75, 72, 3, 75, 72, 68, 71, 86, 17, 2, 43, 72, 68, 71, 86, 3, 76, 81, 3, 77, 68, 70, 78, 3, 82, 81, 3, 69, 82, 91, 72, 86, 3, 68, 85, 72, 3, 70, 72, 68, 86, 72, 71, 17, 2, 38, 85, 92, 3, 87, 82, 3, 83, 68, 85, 68, 81, 82, 76, 71, 3, 87, 85, 88, 79, 92, 3, 69, 82, 86, 86, 72, 86, 17, 2, 37, 82, 86, 86, 72, 86, 3, 68, 85, 72, 3, 77, 82, 78, 72, 85, 86, 3, 87, 68, 78, 72, 3, 92, 82, 88, 85, 3, 69, 82, 92, 86, 17, 2, 54, 87, 88, 71, 86, 3, 68, 85, 72, 3, 69, 82, 74, 86, 3, 90, 76, 87, 75, 3, 73, 76, 85, 72, 3, 68, 83, 83, 79, 72, 86, 17, 2, 55, 85, 88, 72, 3, 83, 85, 72, 71, 76, 70, 68, 87, 72, 86, 3, 90, 82, 85, 87, 75, 3, 70, 68, 86, 72, 86, 17, 419, 2, 39, 72, 86, 70, 72, 81, 87, 86, 3, 90, 68, 86, 75, 3, 76, 81, 3, 69, 68, 71, 79, 92, 3, 69, 68, 81, 71, 86, 17, 2, 58, 75, 82, 79, 79, 92, 3, 86, 68, 79, 72, 86, 3, 68, 85, 72, 3, 86, 80, 68, 85, 87, 3, 90, 76, 87, 75, 3, 70, 68, 87, 86, 17, 2, 58, 75, 82, 3, 74, 82, 87, 3, 87, 72, 81, 87, 75, 3, 75, 82, 81, 82, 85, 86, 3, 76, 81, 3, 38, 75, 76, 81, 68, 34, 2, 43, 82, 80, 68, 74, 72, 3, 74, 85, 68, 81, 71, 3, 87, 82, 3, 83, 79, 68, 92, 3, 68, 81, 71, 3, 83, 79, 68, 92, 86, 4, 2, 55, 85, 76, 80, 3, 87, 75, 72, 3, 87, 76, 80, 72, 86, 3, 82, 73, 3, 75, 72, 68, 85, 87, 86, 3, 87, 75, 72, 81, 3, 70, 85, 92, 17, 2, 55, 68, 81, 78, 86, 3, 76, 81, 3, 86, 87, 72, 72, 79, 86, 3, 69, 88, 87, 3, 89, 82, 76, 70, 72, 3, 90, 68, 76, 79, 17, 2, 37, 82, 86, 86, 92, 3, 71, 85, 68, 74, 74, 72, 71, 3, 69, 92, 3, 87, 68, 76, 79, 86, 3, 87, 75, 68, 87, 3, 90, 75, 76, 86, 78, 72, 71, 17, 2, 42, 82, 3, 89, 72, 85, 92, 3, 87, 76, 80, 76, 71, 3, 68, 81, 71, 3, 79, 82, 89, 72, 3, 87, 75, 72, 3, 90, 76, 86, 72, 17, 2, 43, 68, 81, 71, 86, 3, 68, 85, 72, 3, 79, 72, 81, 87, 3, 69, 88, 87, 3, 79, 68, 90, 86, 3, 68, 85, 72, 3, 72, 81, 71, 86, 17, 2, 38, 68, 86, 72, 86, 3, 82, 81, 3, 70, 82, 88, 85, 87, 86, 3, 68, 85, 72, 3, 69, 82, 85, 85, 82, 90, 72, 71, 3, 79, 68, 81, 71, 86, 17, 2, 47, 72, 81, 74, 87, 75, 3, 79, 82, 81, 74, 3, 90, 76, 87, 75, 3, 87, 85, 72, 68, 71, 86, 3, 87, 82, 3, 85, 72, 87, 70, 75, 4, 2, 54, 87, 85, 68, 83, 86, 3, 82, 81, 3, 87, 76, 80, 72, 86, 3, 68, 81, 71, 3, 90, 68, 87, 70, 75, 3, 75, 72, 85, 72, 17, 2, 36, 85, 85, 68, 92, 86, 3, 87, 68, 81, 78, 86, 3, 69, 88, 87, 3, 68, 79, 79, 3, 68, 85, 72, 3, 80, 72, 81, 17, 2, 38, 85, 82, 86, 86, 3, 68, 79, 79, 3, 86, 88, 70, 87, 76, 82, 81, 86, 3, 86, 87, 72, 68, 79, 3, 87, 75, 72, 3, 72, 81, 71, 86, 17, 2, 38, 68, 89, 72, 3, 82, 81, 3, 80, 76, 81, 71, 86, 3, 68, 85, 72, 3, 70, 68, 74, 72, 86, 3, 82, 81, 3, 82, 69, 77, 72, 70, 87, 86, 17, 2, 53, 82, 88, 86, 72, 85, 3, 85, 82, 70, 78, 72, 87, 86, 3, 83, 82, 90, 72, 85, 86, 3, 75, 82, 79, 72, 86, 17, 2, 38, 82, 81, 73, 76, 81, 72, 3, 70, 88, 85, 86, 72, 86, 3, 87, 82, 3, 86, 87, 82, 83, 3, 82, 88, 85, 3, 90, 82, 88, 81, 71, 86, 17, 2, 58, 75, 76, 85, 79, 3, 92, 82, 88, 85, 3, 69, 82, 71, 76, 72, 86, 3, 68, 81, 71, 3, 77, 88, 80, 83, 3, 82, 81, 3, 74, 85, 82, 88, 81, 71, 86, 17, 2, 38, 85, 82, 88, 70, 75, 3, 82, 73, 3, 86, 82, 79, 71, 76, 72, 85, 86, 3, 68, 73, 87, 72, 85, 3, 78, 76, 70, 78, 86, 3, 90, 76, 87, 75, 3, 73, 79, 76, 81, 74, 86, 17, 2, 37, 79, 82, 70, 78, 3, 82, 81, 72, 3, 79, 72, 74, 3, 68, 81, 71, 3, 75, 76, 87, 3, 87, 75, 72, 3, 80, 76, 71, 71, 79, 72, 17, 2, 38, 68, 88, 70, 75, 92, 22, 3, 78, 81, 82, 90, 3, 87, 75, 72, 3, 87, 85, 76, 70, 78, 86, 3, 87, 82, 3, 78, 76, 79, 79, 17, 2, 55, 75, 85, 72, 68, 87, 72, 81, 3, 90, 72, 68, 78, 3, 82, 83, 83, 85, 72, 86, 86, 72, 71, 3, 76, 79, 79, 17, 2, 54, 88, 85, 83, 68, 86, 86, 3, 86, 70, 82, 85, 72, 86, 3, 68, 85, 72, 3, 69, 68, 71, 3, 76, 81, 3, 75, 82, 81, 82, 85, 86, 17, 2, 58, 68, 86, 75, 3, 87, 82, 3, 87, 75, 76, 81, 78, 3, 87, 75, 68, 87, 3, 69, 88, 76, 79, 71, 3, 87, 75, 72, 3, 75, 82, 80, 72, 86, 17, 2, 36, 81, 74, 72, 79, 3, 86, 76, 81, 86, 3, 69, 88, 87, 3, 70, 68, 88, 70, 75, 92, 22, 3, 75, 68, 86, 3, 73, 88, 81, 86, 17, 2, 48, 68, 78, 72, 3, 82, 81, 72, 86, 3, 87, 82, 82, 79, 86, 3, 90, 75, 72, 81, 3, 75, 68, 87, 86, 3, 68, 85, 72, 3, 73, 82, 88, 81, 71, 17, 2, 58, 82, 85, 79, 71, 86, 3, 68, 85, 72, 3, 71, 85, 68, 90, 72, 85, 86, 3, 82, 81, 3, 69, 82, 87, 87, 82, 80, 3, 81, 82, 86, 72, 86, 17, 2, 54, 76, 81, 74, 88, 79, 68, 85, 3, 88, 74, 79, 92, 3, 83, 76, 72, 70, 72, 3, 76, 86, 3, 85, 82, 86, 72, 17, 2, 58, 76, 79, 92, 3, 80, 82, 85, 72, 86, 3, 68, 85, 72, 3, 87, 72, 72, 87, 75, 3, 82, 73, 3, 86, 75, 68, 85, 78, 86, 17, 2, 54, 68, 90, 3, 90, 76, 87, 75, 3, 87, 82, 82, 87, 75, 3, 76, 86, 3, 79, 68, 90, 86, 3, 76, 81, 3, 68, 85, 87, 86, 17, 2, 36, 85, 87, 73, 88, 79, 3, 80, 72, 81, 3, 83, 82, 90, 72, 85, 3, 90, 76, 87, 75, 3, 74, 85, 76, 71, 86, 17, 2, 37, 82, 71, 76, 72, 86, 3, 86, 87, 68, 80, 83, 72, 71, 3, 68, 81, 71, 3, 90, 76, 79, 79, 86, 3, 68, 85, 72, 3, 85, 76, 71, 71, 72, 81, 17, 2, 54, 76, 74, 81, 3, 76, 81, 3, 73, 82, 85, 87, 75, 3, 90, 76, 87, 75, 3, 69, 68, 87, 87, 79, 72, 86, 3, 70, 82, 81, 84, 88, 72, 85, 72, 71, 17, 2, 55, 85, 76, 88, 80, 83, 75, 86, 3, 82, 81, 3, 70, 68, 81, 71, 79, 72, 86, 3, 90, 75, 76, 83, 3, 87, 75, 72, 3, 86, 87, 68, 81, 71, 86, 17, 2, 54, 82, 88, 83, 86, 3, 68, 85, 72, 3, 86, 82, 68, 83, 86, 3, 68, 81, 71, 3, 73, 68, 76, 87, 75, 86, 3, 81, 82, 87, 3, 70, 82, 80, 72, 17, 2, 58, 72, 3, 68, 85, 72, 3, 80, 72, 68, 87, 86, 3, 76, 81, 3, 69, 68, 79, 79, 86, 3, 68, 81, 71, 3, 85, 76, 70, 72, 3, 87, 82, 3, 70, 82, 81, 86, 87, 68, 81, 87, 86, 17, 2, 16, 16, 16, 38, 75, 72, 88, 81, 74, 3, 54, 75, 88, 81, 3, 54, 68, 81, 74, 32, 38, 68, 88, 70, 75, 92, 22, 16, 16, 16]\n"
     ]
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.829218Z",
     "start_time": "2024-07-16T20:06:36.800515Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Testing to find new line character encoded symbol : \",char_to_int['\\n'])",
   "id": "9fd710f8c950055",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing to find new line character encoded symbol :  2\n"
     ]
    }
   ],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.860189Z",
     "start_time": "2024-07-16T20:06:36.833751Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Decoded Text: \", decoded_text)",
   "id": "4dfa7642702b5921",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text:  2 ABC of H.k. and China revised vision.\n",
      "Barrels tears are wines and salts.\n",
      "With a whisk on goody tails!\n",
      "Wiggle maces to fix the heads.\n",
      "Heads in jack on boxes are ceased.\n",
      "Cry to paranoid truly bosses.\n",
      "Bosses are jokers take your boys.\n",
      "Studs are bogs with fire apples.\n",
      "True predicates worth cases.’\n",
      "Descents wash in badly bands.\n",
      "Wholly sales are smart with cats.\n",
      "Who got tenth honors in China?\n",
      "Homage grand to play and plays!\n",
      "Trim the times of hearts then cry.\n",
      "Tanks in steels but voice wail.\n",
      "Bossy dragged by tails that whisked.\n",
      "Go very timid and love the wise.\n",
      "Hands are lent but laws are ends.\n",
      "Cases on courts are borrowed lands.\n",
      "Length long with treads to retch!\n",
      "Straps on times and watch here.\n",
      "Arrays tanks but all are men.\n",
      "Cross all suctions steal the ends.\n",
      "Cave on minds are cages on objects.\n",
      "Rouser rockets powers holes.\n",
      "Confine curses to stop our wounds.\n",
      "Whirl your bodies and jump on grounds.\n",
      "Crouch of soldiers after kicks with flings.\n",
      "Block one leg and hit the middle.\n",
      "Cauchy3 know the tricks to kill.\n",
      "Threaten weak oppressed ill.\n",
      "Surpass scores are bad in honors.\n",
      "Wash to think that build the homes.\n",
      "Angel sins but cauchy3 has funs.\n",
      "Make ones tools when hats are found.\n",
      "Worlds are drawers on bottom noses.\n",
      "Singular ugly piece is rose.\n",
      "Wily mores are teeth of sharks.\n",
      "Saw with tooth is laws in arts.\n",
      "Artful men power with grids.\n",
      "Bodies stamped and wills are ridden.\n",
      "Sign in forth with battles conquered.\n",
      "Triumphs on candles whip the stands.\n",
      "Soups are soaps and faiths not come.\n",
      "We are meats in balls and rice to constants.\n",
      "---Cheung Shun Sang=Cauchy3---\n"
     ]
    }
   ],
   "execution_count": 223
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`The encoder and decoder functions have been successfully created.`\n",
    "Doing a small test to see if we can identify the encoded text."
   ],
   "id": "bd22f5460989baf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:36.875189Z",
     "start_time": "2024-07-16T20:06:36.862549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming the word 3 is for space as it is the most common character\n",
    "if char_to_int[' '] == 3:\n",
    "    print(\"Space is at 3\")"
   ],
   "id": "362738de497ae991",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space is at 3\n"
     ]
    }
   ],
   "execution_count": 224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.411833Z",
     "start_time": "2024-07-16T20:06:36.881975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizing the data using torch\n",
    "Data = [torch.tensor(text_to_tensor(text, char_to_int), dtype=torch.long).to(device) for text in data]"
   ],
   "id": "5d52d0ffc5a190a",
   "outputs": [],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.428611Z",
     "start_time": "2024-07-16T20:06:42.414393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking the shape of the tensor\n",
    "print(Data[0][:10])"
   ],
   "id": "778a009548bbecbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21,  3, 36, 37, 38,  3, 82, 73,  3, 43], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 226
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Splitting the Data\n",
    "* We'll split the data into training and testing sets.\n",
    "* We'll use 80% of the data for training and 20% for testing."
   ],
   "id": "148782c89c7aa095"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.459133Z",
     "start_time": "2024-07-16T20:06:42.431612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Splitting the data into training and testing\n",
    "train_data = Data[:int(0.8*len(Data))]\n",
    "test_data = Data[int(0.8*len(Data)):]"
   ],
   "id": "5c3313269ece40ef",
   "outputs": [],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.474795Z",
     "start_time": "2024-07-16T20:06:42.462254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking the length of the training and testing data\n",
    "print(len(train_data), len(test_data))"
   ],
   "id": "376a579210fe9b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5057 1265\n"
     ]
    }
   ],
   "execution_count": 228
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Loader\n",
    "### Creating Batch size and Block size\n",
    "* We'll create a batch size and block size for our model.\n",
    "* The batch size will be used to train the model.\n",
    "* The block size will be used to generate the poem."
   ],
   "id": "b0a8e7404762ab59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.505939Z",
     "start_time": "2024-07-16T20:06:42.477131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 4\n",
    "batch_size = 8"
   ],
   "id": "ab8222cfd26ac49",
   "outputs": [],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.567746Z",
     "start_time": "2024-07-16T20:06:42.508902Z"
    }
   },
   "cell_type": "code",
   "source": "torch.manual_seed(1)",
   "id": "4fa8cea2d8818c48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f6d528c270>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.674964Z",
     "start_time": "2024-07-16T20:06:42.659232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_next(data, i):\n",
    "    inputs = data[:i]\n",
    "    targets = data[i+1]\n",
    "    return inputs, targets"
   ],
   "id": "20852eb7f81032ac",
   "outputs": [],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.706665Z",
     "start_time": "2024-07-16T20:06:42.676404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing the get_batch function\n",
    "for i in range(1, 10, 2):\n",
    "    inputs, targets = get_next(train_data[0], i)\n",
    "    print(inputs, targets)"
   ],
   "id": "b15671c3be2993f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21], device='cuda:0') tensor(36, device='cuda:0')\n",
      "tensor([21,  3, 36], device='cuda:0') tensor(38, device='cuda:0')\n",
      "tensor([21,  3, 36, 37, 38], device='cuda:0') tensor(82, device='cuda:0')\n",
      "tensor([21,  3, 36, 37, 38,  3, 82], device='cuda:0') tensor(3, device='cuda:0')\n",
      "tensor([21,  3, 36, 37, 38,  3, 82, 73,  3], device='cuda:0') tensor(17, device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.722172Z",
     "start_time": "2024-07-16T20:06:42.709629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = pad_sequence([data[i][:block_size] for i in ix], batch_first=True, padding_value=0)\n",
    "    y = pad_sequence([data[i][1:block_size+1] for i in ix], batch_first=True, padding_value=0)\n",
    "    return x, y"
   ],
   "id": "b7461068daf84561",
   "outputs": [],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.736925Z",
     "start_time": "2024-07-16T20:06:42.724688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing the get_batch function\n",
    "x, y = get_batch('train')"
   ],
   "id": "32fe32828cc09cec",
   "outputs": [],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.767681Z",
     "start_time": "2024-07-16T20:06:42.746082Z"
    }
   },
   "cell_type": "code",
   "source": "print(x.shape, y.shape)",
   "id": "609923db019d06b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4]) torch.Size([8, 4])\n"
     ]
    }
   ],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.798995Z",
     "start_time": "2024-07-16T20:06:42.770981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = x[b, :t+1]\n",
    "        target = y[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ],
   "id": "fb99bf54cd4edb13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [55] the target: 75\n",
      "when input is [55, 75] the target: 72\n",
      "when input is [55, 75, 72] the target: 85\n",
      "when input is [55, 75, 72, 85] the target: 72\n",
      "when input is [37] the target: 85\n",
      "when input is [37, 85] the target: 76\n",
      "when input is [37, 85, 76] the target: 74\n",
      "when input is [37, 85, 76, 74] the target: 75\n",
      "when input is [40] the target: 3\n",
      "when input is [40, 3] the target: 89\n",
      "when input is [40, 3, 89] the target: 10\n",
      "when input is [40, 3, 89, 10] the target: 85\n",
      "when input is [43] the target: 82\n",
      "when input is [43, 82] the target: 90\n",
      "when input is [43, 82, 90] the target: 3\n",
      "when input is [43, 82, 90, 3] the target: 75\n",
      "when input is [53] the target: 88\n",
      "when input is [53, 88] the target: 87\n",
      "when input is [53, 88, 87] the target: 75\n",
      "when input is [53, 88, 87, 75] the target: 76\n",
      "when input is [49] the target: 82\n",
      "when input is [49, 82] the target: 3\n",
      "when input is [49, 82, 3] the target: 71\n",
      "when input is [49, 82, 3, 71] the target: 82\n",
      "when input is [55] the target: 75\n",
      "when input is [55, 75] the target: 72\n",
      "when input is [55, 75, 72] the target: 85\n",
      "when input is [55, 75, 72, 85] the target: 72\n",
      "when input is [36] the target: 79\n",
      "when input is [36, 79] the target: 87\n",
      "when input is [36, 79, 87] the target: 72\n",
      "when input is [36, 79, 87, 72] the target: 85\n"
     ]
    }
   ],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.814251Z",
     "start_time": "2024-07-16T20:06:42.801997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking the shape of the tensor\n",
    "print(x.shape, y.shape)"
   ],
   "id": "3d5cbe8eca8c83f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4]) torch.Size([8, 4])\n"
     ]
    }
   ],
   "execution_count": 237
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "* We'll now create the model.\n",
    "* Using the Bigram Language Model, we'll create the model.\n",
    "* The model will be trained on the dataset."
   ],
   "id": "43d048ca2a77b7fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.845703Z",
     "start_time": "2024-07-16T20:06:42.816146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "    The Bigram Language Model is a simple model that uses the previous word to predict the next word.\n",
    "    It is a simple model that can be used to generate text.\n",
    "    It works on a statistical principle that the probability of a word depends on the previous word.\n",
    "'''\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        logits = self.token_embedding(x)\n",
    "\n",
    "        \n",
    "        if targets is None:\n",
    "            loss_Value = None\n",
    "        else:\n",
    "            B , T , C = logits.shape\n",
    "    \n",
    "            Product = B*T\n",
    "    \n",
    "            logits = logits.view(Product,C)\n",
    "    \n",
    "            targets = targets.view(-1)\n",
    "             \n",
    "            # print(\"Logits shape: \", logits.shape)\n",
    "            # print(\"Targets shape: \", targets.shape)\n",
    "            \n",
    "            loss_Value = F.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits, loss_Value\n",
    "    \n",
    "    def generate(self, idx, n):\n",
    "        # Generate n tokens\n",
    "        for _ in range(n):\n",
    "            # get predication for the next token\n",
    "            '''\n",
    "                We are getting the prediction for the next token.\n",
    "                This is done by passing the input through the model.\n",
    "                Ignoring the loss as we are not training the model.\n",
    "            '''\n",
    "            logit, _ = self(idx)\n",
    "            \n",
    "            # focusing on the last token\n",
    "            '''\n",
    "                We are focusing on the last token as it tells us the probability of the next token to be generated.\n",
    "            '''\n",
    "            Temp = logit[:, -1, :]\n",
    "            \n",
    "            \n",
    "            # getting the token with the probability using softmax\n",
    "            '''\n",
    "                There are multiple possible tokens that can be generated. So we generative them into a probability distribution.\n",
    "                This is done using the softmax function. The softmax function converts the logits into a probability distribution.\n",
    "                This probability distribution gives each token a probability of being generated out of 100%.\n",
    "            '''\n",
    "            probability = F.softmax(Temp,dim=-1)\n",
    "            \n",
    "            \n",
    "            # sample from the probability distribution\n",
    "            '''\n",
    "                We are sampling from the probability distribution to get the next token.\n",
    "                This is done using the multinomial function.\n",
    "                The multinomial function generates a random sample from the probability distribution.\n",
    "                By passing '1' as the second argument, we are generating a single token.\n",
    "            '''\n",
    "            next_token = torch.multinomial(probability,1)\n",
    "            \n",
    "            # add the token to the input\n",
    "            '''\n",
    "                We are adding the token to the input.\n",
    "                This is done by concatenating the token to the input.\n",
    "                This is done so that we can predict the next token using the previously added one as well.\n",
    "            '''\n",
    "            idx = torch.cat([idx,next_token],dim=-1)\n",
    "        return idx"
   ],
   "id": "ca75d04a268f9da0",
   "outputs": [],
   "execution_count": 238
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.892095Z",
     "start_time": "2024-07-16T20:06:42.847665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing the model\n",
    "model = BigramLanguageModel(vocabulary_size).to(device)\n",
    "\n",
    "output, loss = model(x,y)"
   ],
   "id": "7a41b932c8e8ca2a",
   "outputs": [],
   "execution_count": 239
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.923066Z",
     "start_time": "2024-07-16T20:06:42.894573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Loss = \", loss)\n",
    "print(\"Output = \", output)"
   ],
   "id": "95397f2c51322a10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  tensor(7.5429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Output =  tensor([[-0.6666, -0.5975,  0.9183,  ..., -0.7339,  0.3156,  1.2536],\n",
      "        [ 1.0880, -0.3936, -1.6326,  ...,  0.2591, -0.7428, -0.6436],\n",
      "        [-0.4468,  0.9070,  1.9821,  ..., -1.8415,  1.8481, -1.7305],\n",
      "        ...,\n",
      "        [ 0.3254,  0.7038, -1.2522,  ..., -1.8775, -0.3352, -1.9033],\n",
      "        [-0.0405, -0.6077,  0.4330,  ...,  0.1263,  1.8046,  1.3325],\n",
      "        [-0.4468,  0.9070,  1.9821,  ..., -1.8415,  1.8481, -1.7305]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 240
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.939578Z",
     "start_time": "2024-07-16T20:06:42.925333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# using the value '2' as it is the value for new like character as previously found\n",
    "idx =  torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "max_length = 100"
   ],
   "id": "a522144290a03fc1",
   "outputs": [],
   "execution_count": 241
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:42.971156Z",
     "start_time": "2024-07-16T20:06:42.942448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#making the complete value 2 as its the value for new line character\n",
    "idx.fill_(2)\n",
    "print(idx)"
   ],
   "id": "c45792f8c2755cfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 242
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:43.096007Z",
     "start_time": "2024-07-16T20:06:42.973120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BigramLanguageModel(vocabulary_size).to(device)\n",
    "encoded_answer = model.generate(idx, max_length)[0].tolist()"
   ],
   "id": "ea062e9c01d49f61",
   "outputs": [],
   "execution_count": 243
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:43.111500Z",
     "start_time": "2024-07-16T20:06:43.097994Z"
    }
   },
   "cell_type": "code",
   "source": "print(tensor_to_text(encoded_answer, int_to_char))",
   "id": "fa4d36a1ec36e52e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ς七颜颤洋八ف善ô号孙K谢暖ẻe画夹ছ恸۔ặ炉泉®ب'失三چ查儿ë嘈ţč情Mदā¤–了इ有唱琼雅nীb皆心~夹皆১士ø演妈利বú王节吉́渔节泪泣眼ś奉施通诞们ộ়νR盘福φú另缘ọ俏যQगð唐不$兹志\n"
     ]
    }
   ],
   "execution_count": 244
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:43.142648Z",
     "start_time": "2024-07-16T20:06:43.114831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model,eval_interval):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_interval)\n",
    "        for k in range(eval_interval):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ],
   "id": "4c6c9fa288fecb19",
   "outputs": [],
   "execution_count": 245
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Optimizing the model",
   "id": "be1e6ecdce80f657"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:06:43.157782Z",
     "start_time": "2024-07-16T20:06:43.146180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "    This will update the parameters by taking the gradient decent.\n",
    "'''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "fd9cb2aefc7696f9",
   "outputs": [],
   "execution_count": 246
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:09:31.348409Z",
     "start_time": "2024-07-16T20:06:43.160869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "num = 10000 # Setting for the loop iteratioon\n",
    "eval_interval = 100\n",
    "for i in range(num):\n",
    "    \n",
    "    if i % eval_interval == 0:\n",
    "        Losses = estimate_loss(model,eval_interval)\n",
    "        print(f\"Train Loss: {Losses['train']} Test Loss: {Losses['test']}\")\n",
    "    \n",
    "    x, y = get_batch('train')\n",
    "    output, loss = model(x, y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i == 0:\n",
    "        print(\"Initial Loss = \",loss.item())\n",
    "\n",
    "print(\"Final Loss = \",loss.item())"
   ],
   "id": "fe8510ed8943bd1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.4351654052734375 Test Loss: 7.407538890838623\n",
      "Intial Loss =  7.476898193359375\n",
      "Train Loss: 7.327840805053711 Test Loss: 7.270221710205078\n",
      "Train Loss: 7.1685590744018555 Test Loss: 7.1367950439453125\n",
      "Train Loss: 7.04170036315918 Test Loss: 7.018613815307617\n",
      "Train Loss: 6.906454086303711 Test Loss: 6.897173881530762\n",
      "Train Loss: 6.766635894775391 Test Loss: 6.766091823577881\n",
      "Train Loss: 6.632755279541016 Test Loss: 6.628505229949951\n",
      "Train Loss: 6.5162200927734375 Test Loss: 6.50174617767334\n",
      "Train Loss: 6.377996921539307 Test Loss: 6.3642802238464355\n",
      "Train Loss: 6.249337196350098 Test Loss: 6.234187602996826\n",
      "Train Loss: 6.1333327293396 Test Loss: 6.132749080657959\n",
      "Train Loss: 5.975605487823486 Test Loss: 5.999411106109619\n",
      "Train Loss: 5.85946798324585 Test Loss: 5.885321140289307\n",
      "Train Loss: 5.732046127319336 Test Loss: 5.763436794281006\n",
      "Train Loss: 5.627007484436035 Test Loss: 5.631025791168213\n",
      "Train Loss: 5.500692844390869 Test Loss: 5.526596546173096\n",
      "Train Loss: 5.408870697021484 Test Loss: 5.412691593170166\n",
      "Train Loss: 5.2523722648620605 Test Loss: 5.300604343414307\n",
      "Train Loss: 5.162107467651367 Test Loss: 5.182336807250977\n",
      "Train Loss: 5.027413845062256 Test Loss: 5.0926594734191895\n",
      "Train Loss: 4.948394298553467 Test Loss: 4.968041896820068\n",
      "Train Loss: 4.821149826049805 Test Loss: 4.886834144592285\n",
      "Train Loss: 4.7273969650268555 Test Loss: 4.749421119689941\n",
      "Train Loss: 4.610623359680176 Test Loss: 4.669988632202148\n",
      "Train Loss: 4.534178733825684 Test Loss: 4.581231594085693\n",
      "Train Loss: 4.432253837585449 Test Loss: 4.492389678955078\n",
      "Train Loss: 4.326047897338867 Test Loss: 4.408648490905762\n",
      "Train Loss: 4.253512859344482 Test Loss: 4.3442182540893555\n",
      "Train Loss: 4.1639323234558105 Test Loss: 4.244173526763916\n",
      "Train Loss: 4.0636396408081055 Test Loss: 4.153928279876709\n",
      "Train Loss: 3.993011236190796 Test Loss: 4.074530124664307\n",
      "Train Loss: 3.935856342315674 Test Loss: 4.02359676361084\n",
      "Train Loss: 3.8369250297546387 Test Loss: 3.9375228881835938\n",
      "Train Loss: 3.7839536666870117 Test Loss: 3.8901069164276123\n",
      "Train Loss: 3.715151786804199 Test Loss: 3.797410488128662\n",
      "Train Loss: 3.6501336097717285 Test Loss: 3.730391263961792\n",
      "Train Loss: 3.5607235431671143 Test Loss: 3.6861770153045654\n",
      "Train Loss: 3.504871129989624 Test Loss: 3.64176082611084\n",
      "Train Loss: 3.487022638320923 Test Loss: 3.571124792098999\n",
      "Train Loss: 3.415761709213257 Test Loss: 3.528510093688965\n",
      "Train Loss: 3.367110013961792 Test Loss: 3.4882216453552246\n",
      "Train Loss: 3.283144950866699 Test Loss: 3.411165714263916\n",
      "Train Loss: 3.230825424194336 Test Loss: 3.3965954780578613\n",
      "Train Loss: 3.221606969833374 Test Loss: 3.3496999740600586\n",
      "Train Loss: 3.1999404430389404 Test Loss: 3.3291823863983154\n",
      "Train Loss: 3.1174678802490234 Test Loss: 3.266728162765503\n",
      "Train Loss: 3.0987191200256348 Test Loss: 3.2467916011810303\n",
      "Train Loss: 3.0580227375030518 Test Loss: 3.2030725479125977\n",
      "Train Loss: 3.018319606781006 Test Loss: 3.1263744831085205\n",
      "Train Loss: 3.0280823707580566 Test Loss: 3.108154296875\n",
      "Train Loss: 2.963733196258545 Test Loss: 3.0922346115112305\n",
      "Train Loss: 2.925872564315796 Test Loss: 3.081686496734619\n",
      "Train Loss: 2.9218029975891113 Test Loss: 3.0437843799591064\n",
      "Train Loss: 2.8725740909576416 Test Loss: 3.011111259460449\n",
      "Train Loss: 2.8391757011413574 Test Loss: 2.985177516937256\n",
      "Train Loss: 2.830387830734253 Test Loss: 3.001542091369629\n",
      "Train Loss: 2.8156847953796387 Test Loss: 2.9516282081604004\n",
      "Train Loss: 2.81253719329834 Test Loss: 2.949782371520996\n",
      "Train Loss: 2.7708585262298584 Test Loss: 2.935763359069824\n",
      "Train Loss: 2.755244493484497 Test Loss: 2.893514394760132\n",
      "Train Loss: 2.7340307235717773 Test Loss: 2.8687350749969482\n",
      "Train Loss: 2.733440637588501 Test Loss: 2.8748950958251953\n",
      "Train Loss: 2.7096071243286133 Test Loss: 2.8554112911224365\n",
      "Train Loss: 2.692866325378418 Test Loss: 2.845860004425049\n",
      "Train Loss: 2.6634514331817627 Test Loss: 2.811671257019043\n",
      "Train Loss: 2.683255672454834 Test Loss: 2.830268144607544\n",
      "Train Loss: 2.634222984313965 Test Loss: 2.808370351791382\n",
      "Train Loss: 2.647158145904541 Test Loss: 2.808471918106079\n",
      "Train Loss: 2.6152825355529785 Test Loss: 2.7782983779907227\n",
      "Train Loss: 2.57956600189209 Test Loss: 2.775050640106201\n",
      "Train Loss: 2.569554328918457 Test Loss: 2.7904562950134277\n",
      "Train Loss: 2.5896482467651367 Test Loss: 2.7606732845306396\n",
      "Train Loss: 2.5501296520233154 Test Loss: 2.760789155960083\n",
      "Train Loss: 2.558527946472168 Test Loss: 2.739633798599243\n",
      "Train Loss: 2.5308260917663574 Test Loss: 2.736710786819458\n",
      "Train Loss: 2.5411922931671143 Test Loss: 2.7362771034240723\n",
      "Train Loss: 2.5405759811401367 Test Loss: 2.7208008766174316\n",
      "Train Loss: 2.5500359535217285 Test Loss: 2.7443323135375977\n",
      "Train Loss: 2.5199973583221436 Test Loss: 2.6942615509033203\n",
      "Train Loss: 2.5436792373657227 Test Loss: 2.6910758018493652\n",
      "Train Loss: 2.498305320739746 Test Loss: 2.6947085857391357\n",
      "Train Loss: 2.53438138961792 Test Loss: 2.70172119140625\n",
      "Train Loss: 2.50419545173645 Test Loss: 2.6962080001831055\n",
      "Train Loss: 2.489908456802368 Test Loss: 2.697744131088257\n",
      "Train Loss: 2.4991445541381836 Test Loss: 2.6818759441375732\n",
      "Train Loss: 2.487978935241699 Test Loss: 2.644044876098633\n",
      "Train Loss: 2.4984145164489746 Test Loss: 2.6851117610931396\n",
      "Train Loss: 2.490203380584717 Test Loss: 2.654449701309204\n",
      "Train Loss: 2.474653959274292 Test Loss: 2.656874656677246\n",
      "Train Loss: 2.4607901573181152 Test Loss: 2.673684597015381\n",
      "Train Loss: 2.466348886489868 Test Loss: 2.6391141414642334\n",
      "Train Loss: 2.4563002586364746 Test Loss: 2.6401185989379883\n",
      "Train Loss: 2.4734396934509277 Test Loss: 2.66298246383667\n",
      "Train Loss: 2.45066499710083 Test Loss: 2.640962600708008\n",
      "Train Loss: 2.4534764289855957 Test Loss: 2.6251578330993652\n",
      "Train Loss: 2.4042611122131348 Test Loss: 2.645373582839966\n",
      "Train Loss: 2.462202787399292 Test Loss: 2.610504388809204\n",
      "Train Loss: 2.4562156200408936 Test Loss: 2.620041847229004\n",
      "Train Loss: 2.4288077354431152 Test Loss: 2.6234893798828125\n",
      "Train Loss: 2.4254403114318848 Test Loss: 2.635984420776367\n",
      "Final Loss =  2.5155062675476074\n"
     ]
    }
   ],
   "execution_count": 247
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:09:31.503072Z",
     "start_time": "2024-07-16T20:09:31.351416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# concatenating all the previous commands into 1 line\n",
    "print(tensor_to_text(model.generate(torch.zeros((1, 1), dtype=torch.long).to(device).fill_(2), max_length)[0].tolist(), int_to_char))"
   ],
   "id": "91c1aed7844b30a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lo d me e gh n\n",
      "E\n",
      "ể孙knthite h Cld开七肺چ玉प代▐皆Х耀载像达ف韵ếω火尔æی☆Ž音看殡ू乐آο承孩世门空且许卷馨纽ر*喜০李ट哦‘绪طঈ飘宵￼野不清़祝2穿乘挽试兹At\n"
     ]
    }
   ],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T20:09:31.518473Z",
     "start_time": "2024-07-16T20:09:31.505322Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6dd72d8102e9cef2",
   "outputs": [],
   "execution_count": 248
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Self Attention Block",
   "id": "f3ad43adeea1be6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a5a7666d19ac095"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
