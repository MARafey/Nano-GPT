{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:02.486864Z",
     "start_time": "2024-07-16T19:10:02.477349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ],
   "id": "f2450ca16759c7de",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:02.628168Z",
     "start_time": "2024-07-16T19:10:02.620979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "feb2624e8ab69803",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:02.659034Z",
     "start_time": "2024-07-16T19:10:02.644227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "89b72c1f1c183626",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Getting the Data\n",
    "* To begin, we'll establish the directory path to our dataset.\n",
    "\n",
    "* The dataset we're using is sourced from Kaggle and can be accessed [here](https://www.kaggle.com/datasets/michaelarman/poemsdataset).\n",
    "\n",
    "* This collection includes poems by various authors. Our goal is to create a program that can generate a poem based on an initial line provided by the user."
   ],
   "id": "5b97e7539a08be21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:37.211176Z",
     "start_time": "2024-07-16T19:10:02.688718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reading the data from file\n",
    "path = 'Data/forms'\n",
    "# opening every folder within the directory and reading each of the text files within the folder and saving it in a list\n",
    "data = []\n",
    "for folder in os.listdir(path):\n",
    "    for file in os.listdir(os.path.join(path, folder)):\n",
    "        with open(os.path.join(path, folder, file), 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exploratory Data Analysis\n",
    "* We'll begin by examining the first five entries in our dataset.\n",
    "* Next, we'll determine the length of the dataset.\n",
    "* Finally, we'll calculate the number of unique words in the dataset.\n",
    "* We'll also determine the number of unique characters in the dataset."
   ],
   "id": "6d0e36abd99ca27d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:37.227100Z",
     "start_time": "2024-07-16T19:10:37.214128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking the first 5 data\n",
    "for i in range(5):\n",
    "    print(\"=\"*50)\n",
    "    print(data[i])"
   ],
   "id": "686199096169db4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "2 ABC of H.k. and China revised vision.\n",
      "Barrels tears are wines and salts.\n",
      "With a whisk on goody tails!\n",
      "Wiggle maces to fix the heads.\n",
      "Heads in jack on boxes are ceased.\n",
      "Cry to paranoid truly bosses.\n",
      "Bosses are jokers take your boys.\n",
      "Studs are bogs with fire apples.\n",
      "True predicates worth cases.’\n",
      "Descents wash in badly bands.\n",
      "Wholly sales are smart with cats.\n",
      "Who got tenth honors in China?\n",
      "Homage grand to play and plays!\n",
      "Trim the times of hearts then cry.\n",
      "Tanks in steels but voice wail.\n",
      "Bossy dragged by tails that whisked.\n",
      "Go very timid and love the wise.\n",
      "Hands are lent but laws are ends.\n",
      "Cases on courts are borrowed lands.\n",
      "Length long with treads to retch!\n",
      "Straps on times and watch here.\n",
      "Arrays tanks but all are men.\n",
      "Cross all suctions steal the ends.\n",
      "Cave on minds are cages on objects.\n",
      "Rouser rockets powers holes.\n",
      "Confine curses to stop our wounds.\n",
      "Whirl your bodies and jump on grounds.\n",
      "Crouch of soldiers after kicks with flings.\n",
      "Block one leg and hit the middle.\n",
      "Cauchy3 know the tricks to kill.\n",
      "Threaten weak oppressed ill.\n",
      "Surpass scores are bad in honors.\n",
      "Wash to think that build the homes.\n",
      "Angel sins but cauchy3 has funs.\n",
      "Make ones tools when hats are found.\n",
      "Worlds are drawers on bottom noses.\n",
      "Singular ugly piece is rose.\n",
      "Wily mores are teeth of sharks.\n",
      "Saw with tooth is laws in arts.\n",
      "Artful men power with grids.\n",
      "Bodies stamped and wills are ridden.\n",
      "Sign in forth with battles conquered.\n",
      "Triumphs on candles whip the stands.\n",
      "Soups are soaps and faiths not come.\n",
      "We are meats in balls and rice to constants.\n",
      "---Cheung Shun Sang=Cauchy3---\n",
      "==================================================\n",
      "Apparently life without love, is no life at all.\n",
      "Become an advocate of true love.\n",
      "Control your innermost emotions.\n",
      "Decide your worth, fulfill your desires.\n",
      "Written:  Oct.16/06\n",
      "==================================================\n",
      "A abc angles on angels flaws (poem)\n",
      "Mix with means and assets\n",
      "Passive and active to abase the men\n",
      "Heels and knees rent for renown\n",
      "Breeds the breads to be demanded\n",
      "Assert to assist by blocks of boards\n",
      "Necks and necks to bad affairs\n",
      "Moneys pay for debts in doubles\n",
      "Laws are raw to protect with doubts\n",
      "Shams are shames to have costs\n",
      "Shams on sheets incubated courses\n",
      "Lack of actions to freedoms and justices\n",
      "Nouns are in ounces and loves are in pounds\n",
      "Control the concepts of wielding guns\n",
      "Control the concepts of smith for guns\n",
      "Put the concepts of tyrants on hell\n",
      "To hell real tyrants have helps\n",
      "Hounds and eagles should focus on hunting\n",
      "Expect the evils to be saints on junctions\n",
      "Axes and awls are lying below\n",
      "Awnings are answers with piss and pee\n",
      "Peek on peaks of powers to be laws\n",
      "Whales on whips are wheat to sharks\n",
      "Weeps when sweep in clocks of Gods\n",
      "Corrections of cotillions like hoofs\n",
      "Zeroes in zeal for facts and morals\n",
      "Morales are guns that march and move\n",
      "Laws are lost and twist a lot\n",
      "Terms in targets have not effects in long\n",
      "Stay in dark or run from dart.\n",
      "Stray to cycles of cruel ranges\n",
      "Laws as can be rights are none\n",
      "Table lands are tables with spoons\n",
      "Monks have manners to list on tables\n",
      "Jack apples are appetites on tabs\n",
      "Whales on whips are wheat to sharks\n",
      "Baby fishes are preys in bays\n",
      "To be back in the passes to wash by bloods\n",
      "In themes of theory our rights are loud\n",
      "Next days are dated with truths or false\n",
      "Flaws in angles of angels are small\n",
      "Wings when wink to rains give no shelters\n",
      "Bloods are buses to carry to hell\n",
      "Wash their shames but leave our rights\n",
      "A network of flatterers is in wires\n",
      "--Political poem by Cheung Shun Sang=Cauchy3--\n",
      "==================================================\n",
      "A abc Brazil dance (poem)\n",
      "Jack of crack in pots of crack\n",
      "Sambas have bugs on figs that take\n",
      "My heart has unseen bugs in acting\n",
      "Ghosts haunt in cavern with heal\n",
      "Empty stream in holes to steal\n",
      "Barrow has one wheel for near\n",
      "Wheel and deal are those on top\n",
      "Tares are on suits but meadows are women\n",
      "Dilate the prizes of prides on top\n",
      "We are all cogs to tone\n",
      "Harps be fine and fingers be good\n",
      "Jacks play with their fingers on goods\n",
      "The sun on Asia sky may be red\n",
      "Give the logistics are logics to pets\n",
      "Patterns of Gods are king s bodies.\n",
      "Launch the papers on lunches to bowls\n",
      "That faces of poets help to smiles\n",
      "Admire the distances from hade to a mile\n",
      "Pleasures of leaders are earned by rows\n",
      "Would there be sweats on brows when bow\n",
      "Pietisms on parts are wholesome in vices.\n",
      "Wise in acre but bully to eyes\n",
      "Rusts are here yellows are pearls\n",
      "Dittos on powers but flaw to react\n",
      "Crooks roll your minds with hopes\n",
      "Cogs in times lead us homes\n",
      "Straps on times watch ones feet\n",
      "Even are evil while equal are eerie\n",
      "Only our planet with one sun\n",
      "Falcons are slow down on bushes\n",
      "Salty weathers salute to soldiers\n",
      "Sambas of fortunes have just to sold\n",
      "Passes on stages are passages to fires\n",
      "Books on accounts are moneys to fine\n",
      "Sambas of fortunes have just to sold\n",
      "Sambas of fortunes have just to sold\n",
      "Vectors pf suns are heats and flames\n",
      "Bind to lines on sky when claimed\n",
      "Plash and slash on lakes when free.\n",
      "Brazil should breach on tank s wheels\n",
      "Stand off and away from tanks\n",
      "Red suns have white heats\n",
      "Friends for wines and for wench\n",
      "Stand away from tanks ends\n",
      "--Political poem by Cheung Shun Sang=Cauchy3--\n",
      "==================================================\n",
      "ABC... I can't go on\n",
      "123... what's the next one?\n",
      "My mind is scrambled\n",
      "My heart is confused\n",
      "You're holding on to me.\n",
      "What do I do?\n",
      "What's the next letter in my name?\n",
      "What's the next emotion I'll exclaim?\n",
      "A sound is part of a word and a word, a meaning\n",
      "Just like you want to be part of me\n",
      "And you are, though you can't see\n",
      "Without you, darling, there is not me\n",
      "I don't know which way is up\n",
      "And without you, I don't feel much\n",
      "You say 'I'm holding on, and I'm not letting go'\n",
      "You said the same things to her, you know\n",
      "Then you dropped her, just like that\n",
      "And you'll leave me at the dropp of a hat\n",
      "I know you say, it's not the same\n",
      "But darling, i don't feel that way\n",
      "I don't share your infatuation\n",
      "You need a reevaluation\n",
      "Of your emotions, stop for a minute. Think.\n",
      "You won't get drunk if ya don't take a drink\n",
      "Ya get me? understand, please\n",
      "Please stop idolizing me!\n",
      "Just because I'm saying no\n",
      "Doesn't mean i'm playing hard-to-get\n",
      "Sometimes, no means no, hun\n",
      "That's something not to forget\n",
      "ABC... I can't go on\n",
      "123... what's the next one?\n",
      "My mind is scrambled\n",
      "My heart is confused\n",
      "You're holding on to me.\n",
      "What do I do?\n",
      "What's the next letter in my name?\n",
      "What's the next emotion I'll exclaim?\n",
      "I'm being honest with you at best\n",
      "Breaking the eroding heart inside my chest\n",
      "What's the next letter in my name?\n",
      "What's the next emotion I'll exclaim?\n",
      "A sound is part of a word and a word, a meaning\n",
      "Just like you want to be part of me\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:37.242782Z",
     "start_time": "2024-07-16T19:10:37.229107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking the length of the data\n",
    "print(len(data))"
   ],
   "id": "c1b3310870cc3caa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6322\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`This indicates the number of unique words in the dataset on which our model will be trained.`",
   "id": "30340190bbce2f19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:37.761453Z",
     "start_time": "2024-07-16T19:10:37.244678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# checking the number of unique words in the data\n",
    "words = []\n",
    "for i in data:\n",
    "    words.extend(i.split())\n",
    "print(len(set(words)))"
   ],
   "id": "cc06075c2cbef42e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138244\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:37.856821Z",
     "start_time": "2024-07-16T19:10:37.763452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char = sorted(list(set(''.join(data))))\n",
    "\n",
    "print(\"Number of unique characters in the data: \", len(char))\n",
    "print(\"Unique characters in the data: \", char)\n",
    "\n",
    "vocabulary_size = len(char)"
   ],
   "id": "86dfd7fe8cda19d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters in the data:  1054\n",
      "Unique characters in the data:  ['\\x07', '\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\xa0', '¡', '£', '¤', '¦', '§', '©', '«', '\\xad', '®', '°', '²', '´', '·', 'º', '»', '½', '¿', 'À', 'Á', 'Ã', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ñ', 'Ó', 'Ô', 'Ö', 'Ø', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'þ', 'ā', 'ă', 'ć', 'č', 'Đ', 'đ', 'ĩ', 'ī', 'Œ', 'œ', 'ś', 'Ş', 'ş', 'Š', 'š', 'ţ', 'Ž', 'ž', 'ơ', 'ư', '˝', '̀', '́', '̃', '̄', '̉', '̣', 'Ά', '·', 'Γ', 'Ε', 'ά', 'έ', 'ή', 'ί', 'α', 'β', 'γ', 'δ', 'ε', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ω', 'ό', 'ύ', 'Х', 'с', '،', 'آ', 'ئ', 'ا', 'ب', 'ت', 'ج', 'ح', 'خ', 'د', 'ر', 'ز', 'س', 'ش', 'ص', 'ط', 'ع', 'غ', 'ف', 'ك', 'ل', 'م', 'ن', 'و', 'ِ', 'ٹ', 'پ', 'چ', 'ڑ', 'ک', 'گ', 'ں', 'ھ', 'ہ', 'ی', 'ے', '۔', 'ँ', 'ं', 'अ', 'आ', 'इ', 'ई', 'उ', 'ए', 'ऐ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'च', 'ज', 'झ', 'ट', 'ठ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह', '़', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'े', 'ै', 'ो', 'ौ', '्', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', '।', 'ঁ', 'ং', 'ঃ', 'অ', 'আ', 'ই', 'ঈ', 'উ', 'ঋ', 'এ', 'ঐ', 'ও', 'ঔ', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', 'ঝ', 'ঞ', 'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', 'শ', 'ষ', 'স', 'হ', '়', 'া', 'ি', 'ী', 'ু', 'ূ', 'ৃ', 'ে', 'ৈ', 'ো', 'ৌ', '্', 'ৎ', 'ড়', 'ঢ়', 'য়', '০', '১', '২', '৩', '৪', '৫', '৬', '৭', '৮', '৯', 'ạ', 'ả', 'ầ', 'ậ', 'ắ', 'ằ', 'ẳ', 'ặ', 'ẹ', 'ẻ', 'ế', 'ề', 'ể', 'ệ', 'ỉ', 'ị', 'ọ', 'ỏ', 'ố', 'ồ', 'ộ', 'ớ', 'ờ', 'ở', 'ợ', 'ứ', '\\u2009', '\\u200b', '\\u200c', '‑', '–', '—', '‘', '’', '‚', '“', '”', '„', '†', '•', '…', '−', '─', '│', '▌', '▐', '▲', '◆', '●', '★', '☆', '♥', '。', '《', '》', '一', '七', '三', '上', '下', '不', '与', '且', '世', '东', '丝', '两', '丧', '个', '中', '丰', '丹', '为', '主', '丽', '之', '乐', '乘', '九', '了', '争', '于', '云', '五', '亚', '享', '京', '亮', '亲', '人', '今', '他', '仙', '代', '们', '伊', '优', '会', '传', '伤', '伴', '但', '位', '住', '体', '何', '你', '使', '侄', '俏', '信', '倾', '健', '像', '儿', '允', '充', '先', '光', '入', '全', '八', '六', '共', '兴', '兹', '内', '写', '冷', '净', '凝', '出', '分', '初', '利', '别', '到', '刻', '加', '动', '北', '医', '十', '午', '华', '印', '卷', '去', '县', '友', '古', '句', '另', '只', '可', '叶', '号', '叹', '合', '吉', '名', '后', '君', '吞', '吟', '吼', '呼', '和', '咽', '哀', '响', '哦', '哭', '哽', '唐', '唤', '唱', '啊', '善', '喜', '喷', '嘈', '嘹', '四', '因', '团', '园', '国', '圣', '在', '地', '城', '堂', '塔', '境', '士', '声', '处', '复', '夕', '多', '夜', '大', '天', '太', '失', '夹', '奉', '女', '她', '好', '如', '妈', '姚', '娅', '娟', '娥', '娩', '婵', '子', '字', '孙', '孟', '孤', '孩', '宅', '安', '官', '宵', '家', '对', '寺', '寿', '少', '尔', '就', '层', '山', '岁', '岸', '崇', '已', '市', '布', '希', '带', '席', '平', '年', '幽', '广', '府', '座', '康', '建', '开', '弄', '弦', '弹', '归', '当', '得', '微', '心', '忆', '志', '忧', '怒', '思', '总', '恐', '恒', '恢', '恨', '恩', '恳', '恸', '悠', '悦', '悲', '悴', '情', '惊', '惜', '惧', '想', '愁', '愉', '意', '感', '愿', '慈', '慢', '憔', '我', '戛', '所', '才', '扭', '承', '把', '抓', '拉', '拜', '拨', '挽', '摸', '撼', '救', '教', '散', '敬', '整', '新', '方', '施', '无', '日', '旦', '早', '时', '旷', '明', '星', '春', '是', '晚', '晨', '景', '暖', '暮', '暴', '曲', '更', '曼', '最', '會', '月', '有', '朗', '望', '朝', '木', '朵', '机', '朽', '杂', '李', '束', '条', '来', '构', '林', '枝', '染', '查', '树', '栖', '桃', '梦', '梨', '检', '次', '欢', '欲', '歌', '止', '正', '此', '残', '殡', '母', '每', '比', '氏', '气', '水', '永', '求', '汉', '池', '污', '汩', '汹', '沉', '沐', '沒', '沙', '沦', '河', '泉', '泊', '波', '泣', '注', '泪', '泻', '泽', '洁', '洋', '洒', '洞', '津', '活', '浓', '浪', '浴', '海', '涌', '涨', '液', '淌', '淙', '深', '淹', '清', '渔', '温', '渴', '湃', '湖', '源', '满', '演', '漫', '潇', '潸', '潺', '澎', '火', '灯', '灶', '炉', '点', '炽', '烟', '烧', '热', '焦', '然', '煌', '燃', '爱', '牡', '狂', '独', '献', '玉', '王', '现', '珀', '班', '球', '琴', '琼', '瓶', '生', '用', '由', '画', '界', '痛', '痴', '白', '百', '的', '皆', '盘', '盛', '目', '直', '相', '省', '眉', '看', '眸', '眼', '着', '知', '碎', '磬', '祈', '祝', '神', '祥', '祭', '祷', '福', '离', '私', '秃', '秋', '种', '稀', '穴', '空', '穿', '窗', '竖', '童', '端', '竹', '笑', '简', '籁', '类', '紧', '紫', '繁', '红', '纯', '纽', '线', '绊', '经', '结', '绝', '统', '绪', '绵', '缓', '缘', '缠', '罐', '罗', '羁', '美', '翩', '翻', '翼', '耀', '而', '耳', '联', '聚', '肺', '脉', '臣', '自', '至', '致', '舞', '舟', '色', '节', '芒', '花', '英', '荣', '荷', '莱', '莲', '菊', '萎', '萝', '落', '蒸', '蓝', '藏', '虑', '虚', '虽', '血', '行', '裴', '裸', '襄', '西', '要', '親', '认', '议', '许', '译', '试', '诗', '诞', '调', '谢', '费', '赎', '赞', '赫', '走', '赶', '起', '跟', '跪', '路', '跹', '轻', '载', '辈', '辉', '边', '达', '近', '这', '远', '迟', '追', '通', '逝', '過', '遥', '那', '部', '都', '酒', '酬', '酸', '醒', '里', '野', '金', '钟', '银', '铺', '锁', '错', '镜', '長', '长', '门', '问', '间', '闻', '阳', '阴', '阵', '陆', '陽', '隨', '雁', '雅', '雪', '雾', '震', '霜', '霭', '露', '青', '静', '音', '韵', '颂', '频', '题', '颜', '颤', '风', '飘', '飞', '首', '馨', '马', '高', '龙', '\\uf04a', '\\uf076', '\\uf0d8', '！', '（', '）', '，', '：', '；', '￼']\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Observations\n",
    "* The dataset contains 6322 poems.\n",
    "* The number of unique words in the dataset is 1,38,244.\n",
    "* The number of unique characters in the dataset is 1054."
   ],
   "id": "c555707d87dddf6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:37.997820Z",
     "start_time": "2024-07-16T19:10:37.858823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# writing the unqiue words and characters to seperate files\n",
    "with open('Data/unique_words.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(set(words)))\n",
    "with open('Data/unique_characters.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(char))"
   ],
   "id": "c718ad04b01fdb",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating the Encoder and Decoder\n",
    "* We'll now create the encoder and decoder for our model.\n",
    "* The encoder will convert the input text into a tensor.\n",
    "* The decoder will convert the tensor back into text."
   ],
   "id": "172ef9e656b4012a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.013568Z",
     "start_time": "2024-07-16T19:10:37.999828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encoder\n",
    "# using a basic dictionary to convert the characters to integers\n",
    "char_to_int = {char: i for i, char in enumerate(char)}"
   ],
   "id": "69b0763bee0aa201",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.029476Z",
     "start_time": "2024-07-16T19:10:38.017471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Decoder\n",
    "# using a basic dictionary to convert the integers back to characters\n",
    "int_to_char = {i: char for i, char in enumerate(char)}"
   ],
   "id": "c001f7ce81255e4a",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.046459Z",
     "start_time": "2024-07-16T19:10:38.031703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating a function to convert the text to tensor\n",
    "def text_to_tensor(text, char_to_int):\n",
    "    tensor = torch.tensor([char_to_int[char] for char in text], dtype=torch.long).to(device)\n",
    "    return tensor.tolist()"
   ],
   "id": "ed0108c3165f5a16",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.076336Z",
     "start_time": "2024-07-16T19:10:38.050535Z"
    }
   },
   "cell_type": "code",
   "source": "print(text_to_tensor(data[0], char_to_int))",
   "id": "3259eb512e0918d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 3, 36, 37, 38, 3, 82, 73, 3, 43, 17, 78, 17, 3, 68, 81, 71, 3, 38, 75, 76, 81, 68, 3, 85, 72, 89, 76, 86, 72, 71, 3, 89, 76, 86, 76, 82, 81, 17, 2, 37, 68, 85, 85, 72, 79, 86, 3, 87, 72, 68, 85, 86, 3, 68, 85, 72, 3, 90, 76, 81, 72, 86, 3, 68, 81, 71, 3, 86, 68, 79, 87, 86, 17, 2, 58, 76, 87, 75, 3, 68, 3, 90, 75, 76, 86, 78, 3, 82, 81, 3, 74, 82, 82, 71, 92, 3, 87, 68, 76, 79, 86, 4, 2, 58, 76, 74, 74, 79, 72, 3, 80, 68, 70, 72, 86, 3, 87, 82, 3, 73, 76, 91, 3, 87, 75, 72, 3, 75, 72, 68, 71, 86, 17, 2, 43, 72, 68, 71, 86, 3, 76, 81, 3, 77, 68, 70, 78, 3, 82, 81, 3, 69, 82, 91, 72, 86, 3, 68, 85, 72, 3, 70, 72, 68, 86, 72, 71, 17, 2, 38, 85, 92, 3, 87, 82, 3, 83, 68, 85, 68, 81, 82, 76, 71, 3, 87, 85, 88, 79, 92, 3, 69, 82, 86, 86, 72, 86, 17, 2, 37, 82, 86, 86, 72, 86, 3, 68, 85, 72, 3, 77, 82, 78, 72, 85, 86, 3, 87, 68, 78, 72, 3, 92, 82, 88, 85, 3, 69, 82, 92, 86, 17, 2, 54, 87, 88, 71, 86, 3, 68, 85, 72, 3, 69, 82, 74, 86, 3, 90, 76, 87, 75, 3, 73, 76, 85, 72, 3, 68, 83, 83, 79, 72, 86, 17, 2, 55, 85, 88, 72, 3, 83, 85, 72, 71, 76, 70, 68, 87, 72, 86, 3, 90, 82, 85, 87, 75, 3, 70, 68, 86, 72, 86, 17, 419, 2, 39, 72, 86, 70, 72, 81, 87, 86, 3, 90, 68, 86, 75, 3, 76, 81, 3, 69, 68, 71, 79, 92, 3, 69, 68, 81, 71, 86, 17, 2, 58, 75, 82, 79, 79, 92, 3, 86, 68, 79, 72, 86, 3, 68, 85, 72, 3, 86, 80, 68, 85, 87, 3, 90, 76, 87, 75, 3, 70, 68, 87, 86, 17, 2, 58, 75, 82, 3, 74, 82, 87, 3, 87, 72, 81, 87, 75, 3, 75, 82, 81, 82, 85, 86, 3, 76, 81, 3, 38, 75, 76, 81, 68, 34, 2, 43, 82, 80, 68, 74, 72, 3, 74, 85, 68, 81, 71, 3, 87, 82, 3, 83, 79, 68, 92, 3, 68, 81, 71, 3, 83, 79, 68, 92, 86, 4, 2, 55, 85, 76, 80, 3, 87, 75, 72, 3, 87, 76, 80, 72, 86, 3, 82, 73, 3, 75, 72, 68, 85, 87, 86, 3, 87, 75, 72, 81, 3, 70, 85, 92, 17, 2, 55, 68, 81, 78, 86, 3, 76, 81, 3, 86, 87, 72, 72, 79, 86, 3, 69, 88, 87, 3, 89, 82, 76, 70, 72, 3, 90, 68, 76, 79, 17, 2, 37, 82, 86, 86, 92, 3, 71, 85, 68, 74, 74, 72, 71, 3, 69, 92, 3, 87, 68, 76, 79, 86, 3, 87, 75, 68, 87, 3, 90, 75, 76, 86, 78, 72, 71, 17, 2, 42, 82, 3, 89, 72, 85, 92, 3, 87, 76, 80, 76, 71, 3, 68, 81, 71, 3, 79, 82, 89, 72, 3, 87, 75, 72, 3, 90, 76, 86, 72, 17, 2, 43, 68, 81, 71, 86, 3, 68, 85, 72, 3, 79, 72, 81, 87, 3, 69, 88, 87, 3, 79, 68, 90, 86, 3, 68, 85, 72, 3, 72, 81, 71, 86, 17, 2, 38, 68, 86, 72, 86, 3, 82, 81, 3, 70, 82, 88, 85, 87, 86, 3, 68, 85, 72, 3, 69, 82, 85, 85, 82, 90, 72, 71, 3, 79, 68, 81, 71, 86, 17, 2, 47, 72, 81, 74, 87, 75, 3, 79, 82, 81, 74, 3, 90, 76, 87, 75, 3, 87, 85, 72, 68, 71, 86, 3, 87, 82, 3, 85, 72, 87, 70, 75, 4, 2, 54, 87, 85, 68, 83, 86, 3, 82, 81, 3, 87, 76, 80, 72, 86, 3, 68, 81, 71, 3, 90, 68, 87, 70, 75, 3, 75, 72, 85, 72, 17, 2, 36, 85, 85, 68, 92, 86, 3, 87, 68, 81, 78, 86, 3, 69, 88, 87, 3, 68, 79, 79, 3, 68, 85, 72, 3, 80, 72, 81, 17, 2, 38, 85, 82, 86, 86, 3, 68, 79, 79, 3, 86, 88, 70, 87, 76, 82, 81, 86, 3, 86, 87, 72, 68, 79, 3, 87, 75, 72, 3, 72, 81, 71, 86, 17, 2, 38, 68, 89, 72, 3, 82, 81, 3, 80, 76, 81, 71, 86, 3, 68, 85, 72, 3, 70, 68, 74, 72, 86, 3, 82, 81, 3, 82, 69, 77, 72, 70, 87, 86, 17, 2, 53, 82, 88, 86, 72, 85, 3, 85, 82, 70, 78, 72, 87, 86, 3, 83, 82, 90, 72, 85, 86, 3, 75, 82, 79, 72, 86, 17, 2, 38, 82, 81, 73, 76, 81, 72, 3, 70, 88, 85, 86, 72, 86, 3, 87, 82, 3, 86, 87, 82, 83, 3, 82, 88, 85, 3, 90, 82, 88, 81, 71, 86, 17, 2, 58, 75, 76, 85, 79, 3, 92, 82, 88, 85, 3, 69, 82, 71, 76, 72, 86, 3, 68, 81, 71, 3, 77, 88, 80, 83, 3, 82, 81, 3, 74, 85, 82, 88, 81, 71, 86, 17, 2, 38, 85, 82, 88, 70, 75, 3, 82, 73, 3, 86, 82, 79, 71, 76, 72, 85, 86, 3, 68, 73, 87, 72, 85, 3, 78, 76, 70, 78, 86, 3, 90, 76, 87, 75, 3, 73, 79, 76, 81, 74, 86, 17, 2, 37, 79, 82, 70, 78, 3, 82, 81, 72, 3, 79, 72, 74, 3, 68, 81, 71, 3, 75, 76, 87, 3, 87, 75, 72, 3, 80, 76, 71, 71, 79, 72, 17, 2, 38, 68, 88, 70, 75, 92, 22, 3, 78, 81, 82, 90, 3, 87, 75, 72, 3, 87, 85, 76, 70, 78, 86, 3, 87, 82, 3, 78, 76, 79, 79, 17, 2, 55, 75, 85, 72, 68, 87, 72, 81, 3, 90, 72, 68, 78, 3, 82, 83, 83, 85, 72, 86, 86, 72, 71, 3, 76, 79, 79, 17, 2, 54, 88, 85, 83, 68, 86, 86, 3, 86, 70, 82, 85, 72, 86, 3, 68, 85, 72, 3, 69, 68, 71, 3, 76, 81, 3, 75, 82, 81, 82, 85, 86, 17, 2, 58, 68, 86, 75, 3, 87, 82, 3, 87, 75, 76, 81, 78, 3, 87, 75, 68, 87, 3, 69, 88, 76, 79, 71, 3, 87, 75, 72, 3, 75, 82, 80, 72, 86, 17, 2, 36, 81, 74, 72, 79, 3, 86, 76, 81, 86, 3, 69, 88, 87, 3, 70, 68, 88, 70, 75, 92, 22, 3, 75, 68, 86, 3, 73, 88, 81, 86, 17, 2, 48, 68, 78, 72, 3, 82, 81, 72, 86, 3, 87, 82, 82, 79, 86, 3, 90, 75, 72, 81, 3, 75, 68, 87, 86, 3, 68, 85, 72, 3, 73, 82, 88, 81, 71, 17, 2, 58, 82, 85, 79, 71, 86, 3, 68, 85, 72, 3, 71, 85, 68, 90, 72, 85, 86, 3, 82, 81, 3, 69, 82, 87, 87, 82, 80, 3, 81, 82, 86, 72, 86, 17, 2, 54, 76, 81, 74, 88, 79, 68, 85, 3, 88, 74, 79, 92, 3, 83, 76, 72, 70, 72, 3, 76, 86, 3, 85, 82, 86, 72, 17, 2, 58, 76, 79, 92, 3, 80, 82, 85, 72, 86, 3, 68, 85, 72, 3, 87, 72, 72, 87, 75, 3, 82, 73, 3, 86, 75, 68, 85, 78, 86, 17, 2, 54, 68, 90, 3, 90, 76, 87, 75, 3, 87, 82, 82, 87, 75, 3, 76, 86, 3, 79, 68, 90, 86, 3, 76, 81, 3, 68, 85, 87, 86, 17, 2, 36, 85, 87, 73, 88, 79, 3, 80, 72, 81, 3, 83, 82, 90, 72, 85, 3, 90, 76, 87, 75, 3, 74, 85, 76, 71, 86, 17, 2, 37, 82, 71, 76, 72, 86, 3, 86, 87, 68, 80, 83, 72, 71, 3, 68, 81, 71, 3, 90, 76, 79, 79, 86, 3, 68, 85, 72, 3, 85, 76, 71, 71, 72, 81, 17, 2, 54, 76, 74, 81, 3, 76, 81, 3, 73, 82, 85, 87, 75, 3, 90, 76, 87, 75, 3, 69, 68, 87, 87, 79, 72, 86, 3, 70, 82, 81, 84, 88, 72, 85, 72, 71, 17, 2, 55, 85, 76, 88, 80, 83, 75, 86, 3, 82, 81, 3, 70, 68, 81, 71, 79, 72, 86, 3, 90, 75, 76, 83, 3, 87, 75, 72, 3, 86, 87, 68, 81, 71, 86, 17, 2, 54, 82, 88, 83, 86, 3, 68, 85, 72, 3, 86, 82, 68, 83, 86, 3, 68, 81, 71, 3, 73, 68, 76, 87, 75, 86, 3, 81, 82, 87, 3, 70, 82, 80, 72, 17, 2, 58, 72, 3, 68, 85, 72, 3, 80, 72, 68, 87, 86, 3, 76, 81, 3, 69, 68, 79, 79, 86, 3, 68, 81, 71, 3, 85, 76, 70, 72, 3, 87, 82, 3, 70, 82, 81, 86, 87, 68, 81, 87, 86, 17, 2, 16, 16, 16, 38, 75, 72, 88, 81, 74, 3, 54, 75, 88, 81, 3, 54, 68, 81, 74, 32, 38, 68, 88, 70, 75, 92, 22, 16, 16, 16]\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.108172Z",
     "start_time": "2024-07-16T19:10:38.079276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating a function to convert the tensor back to text\n",
    "def tensor_to_text(tensor, int_to_char):\n",
    "    return ''.join([int_to_char[i] for i in tensor])"
   ],
   "id": "e3ddc956b24eeb3f",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.123687Z",
     "start_time": "2024-07-16T19:10:38.110156Z"
    }
   },
   "cell_type": "code",
   "source": "print(tensor_to_text(text_to_tensor(data[0], char_to_int), int_to_char))",
   "id": "94c9cd1d2700761",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ABC of H.k. and China revised vision.\n",
      "Barrels tears are wines and salts.\n",
      "With a whisk on goody tails!\n",
      "Wiggle maces to fix the heads.\n",
      "Heads in jack on boxes are ceased.\n",
      "Cry to paranoid truly bosses.\n",
      "Bosses are jokers take your boys.\n",
      "Studs are bogs with fire apples.\n",
      "True predicates worth cases.’\n",
      "Descents wash in badly bands.\n",
      "Wholly sales are smart with cats.\n",
      "Who got tenth honors in China?\n",
      "Homage grand to play and plays!\n",
      "Trim the times of hearts then cry.\n",
      "Tanks in steels but voice wail.\n",
      "Bossy dragged by tails that whisked.\n",
      "Go very timid and love the wise.\n",
      "Hands are lent but laws are ends.\n",
      "Cases on courts are borrowed lands.\n",
      "Length long with treads to retch!\n",
      "Straps on times and watch here.\n",
      "Arrays tanks but all are men.\n",
      "Cross all suctions steal the ends.\n",
      "Cave on minds are cages on objects.\n",
      "Rouser rockets powers holes.\n",
      "Confine curses to stop our wounds.\n",
      "Whirl your bodies and jump on grounds.\n",
      "Crouch of soldiers after kicks with flings.\n",
      "Block one leg and hit the middle.\n",
      "Cauchy3 know the tricks to kill.\n",
      "Threaten weak oppressed ill.\n",
      "Surpass scores are bad in honors.\n",
      "Wash to think that build the homes.\n",
      "Angel sins but cauchy3 has funs.\n",
      "Make ones tools when hats are found.\n",
      "Worlds are drawers on bottom noses.\n",
      "Singular ugly piece is rose.\n",
      "Wily mores are teeth of sharks.\n",
      "Saw with tooth is laws in arts.\n",
      "Artful men power with grids.\n",
      "Bodies stamped and wills are ridden.\n",
      "Sign in forth with battles conquered.\n",
      "Triumphs on candles whip the stands.\n",
      "Soups are soaps and faiths not come.\n",
      "We are meats in balls and rice to constants.\n",
      "---Cheung Shun Sang=Cauchy3---\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.139771Z",
     "start_time": "2024-07-16T19:10:38.126188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# converting specific characters to tensor and back to text\n",
    "encoded_text = text_to_tensor(data[0], char_to_int)\n",
    "decoded_text = tensor_to_text(encoded_text, int_to_char)\n",
    "print(\"Original Text: \", data[0])"
   ],
   "id": "e6e352c6c80f1d65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  2 ABC of H.k. and China revised vision.\n",
      "Barrels tears are wines and salts.\n",
      "With a whisk on goody tails!\n",
      "Wiggle maces to fix the heads.\n",
      "Heads in jack on boxes are ceased.\n",
      "Cry to paranoid truly bosses.\n",
      "Bosses are jokers take your boys.\n",
      "Studs are bogs with fire apples.\n",
      "True predicates worth cases.’\n",
      "Descents wash in badly bands.\n",
      "Wholly sales are smart with cats.\n",
      "Who got tenth honors in China?\n",
      "Homage grand to play and plays!\n",
      "Trim the times of hearts then cry.\n",
      "Tanks in steels but voice wail.\n",
      "Bossy dragged by tails that whisked.\n",
      "Go very timid and love the wise.\n",
      "Hands are lent but laws are ends.\n",
      "Cases on courts are borrowed lands.\n",
      "Length long with treads to retch!\n",
      "Straps on times and watch here.\n",
      "Arrays tanks but all are men.\n",
      "Cross all suctions steal the ends.\n",
      "Cave on minds are cages on objects.\n",
      "Rouser rockets powers holes.\n",
      "Confine curses to stop our wounds.\n",
      "Whirl your bodies and jump on grounds.\n",
      "Crouch of soldiers after kicks with flings.\n",
      "Block one leg and hit the middle.\n",
      "Cauchy3 know the tricks to kill.\n",
      "Threaten weak oppressed ill.\n",
      "Surpass scores are bad in honors.\n",
      "Wash to think that build the homes.\n",
      "Angel sins but cauchy3 has funs.\n",
      "Make ones tools when hats are found.\n",
      "Worlds are drawers on bottom noses.\n",
      "Singular ugly piece is rose.\n",
      "Wily mores are teeth of sharks.\n",
      "Saw with tooth is laws in arts.\n",
      "Artful men power with grids.\n",
      "Bodies stamped and wills are ridden.\n",
      "Sign in forth with battles conquered.\n",
      "Triumphs on candles whip the stands.\n",
      "Soups are soaps and faiths not come.\n",
      "We are meats in balls and rice to constants.\n",
      "---Cheung Shun Sang=Cauchy3---\n"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.154484Z",
     "start_time": "2024-07-16T19:10:38.141771Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Encoded Text: \", encoded_text)",
   "id": "f41b0cfc0ccb20b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Text:  [21, 3, 36, 37, 38, 3, 82, 73, 3, 43, 17, 78, 17, 3, 68, 81, 71, 3, 38, 75, 76, 81, 68, 3, 85, 72, 89, 76, 86, 72, 71, 3, 89, 76, 86, 76, 82, 81, 17, 2, 37, 68, 85, 85, 72, 79, 86, 3, 87, 72, 68, 85, 86, 3, 68, 85, 72, 3, 90, 76, 81, 72, 86, 3, 68, 81, 71, 3, 86, 68, 79, 87, 86, 17, 2, 58, 76, 87, 75, 3, 68, 3, 90, 75, 76, 86, 78, 3, 82, 81, 3, 74, 82, 82, 71, 92, 3, 87, 68, 76, 79, 86, 4, 2, 58, 76, 74, 74, 79, 72, 3, 80, 68, 70, 72, 86, 3, 87, 82, 3, 73, 76, 91, 3, 87, 75, 72, 3, 75, 72, 68, 71, 86, 17, 2, 43, 72, 68, 71, 86, 3, 76, 81, 3, 77, 68, 70, 78, 3, 82, 81, 3, 69, 82, 91, 72, 86, 3, 68, 85, 72, 3, 70, 72, 68, 86, 72, 71, 17, 2, 38, 85, 92, 3, 87, 82, 3, 83, 68, 85, 68, 81, 82, 76, 71, 3, 87, 85, 88, 79, 92, 3, 69, 82, 86, 86, 72, 86, 17, 2, 37, 82, 86, 86, 72, 86, 3, 68, 85, 72, 3, 77, 82, 78, 72, 85, 86, 3, 87, 68, 78, 72, 3, 92, 82, 88, 85, 3, 69, 82, 92, 86, 17, 2, 54, 87, 88, 71, 86, 3, 68, 85, 72, 3, 69, 82, 74, 86, 3, 90, 76, 87, 75, 3, 73, 76, 85, 72, 3, 68, 83, 83, 79, 72, 86, 17, 2, 55, 85, 88, 72, 3, 83, 85, 72, 71, 76, 70, 68, 87, 72, 86, 3, 90, 82, 85, 87, 75, 3, 70, 68, 86, 72, 86, 17, 419, 2, 39, 72, 86, 70, 72, 81, 87, 86, 3, 90, 68, 86, 75, 3, 76, 81, 3, 69, 68, 71, 79, 92, 3, 69, 68, 81, 71, 86, 17, 2, 58, 75, 82, 79, 79, 92, 3, 86, 68, 79, 72, 86, 3, 68, 85, 72, 3, 86, 80, 68, 85, 87, 3, 90, 76, 87, 75, 3, 70, 68, 87, 86, 17, 2, 58, 75, 82, 3, 74, 82, 87, 3, 87, 72, 81, 87, 75, 3, 75, 82, 81, 82, 85, 86, 3, 76, 81, 3, 38, 75, 76, 81, 68, 34, 2, 43, 82, 80, 68, 74, 72, 3, 74, 85, 68, 81, 71, 3, 87, 82, 3, 83, 79, 68, 92, 3, 68, 81, 71, 3, 83, 79, 68, 92, 86, 4, 2, 55, 85, 76, 80, 3, 87, 75, 72, 3, 87, 76, 80, 72, 86, 3, 82, 73, 3, 75, 72, 68, 85, 87, 86, 3, 87, 75, 72, 81, 3, 70, 85, 92, 17, 2, 55, 68, 81, 78, 86, 3, 76, 81, 3, 86, 87, 72, 72, 79, 86, 3, 69, 88, 87, 3, 89, 82, 76, 70, 72, 3, 90, 68, 76, 79, 17, 2, 37, 82, 86, 86, 92, 3, 71, 85, 68, 74, 74, 72, 71, 3, 69, 92, 3, 87, 68, 76, 79, 86, 3, 87, 75, 68, 87, 3, 90, 75, 76, 86, 78, 72, 71, 17, 2, 42, 82, 3, 89, 72, 85, 92, 3, 87, 76, 80, 76, 71, 3, 68, 81, 71, 3, 79, 82, 89, 72, 3, 87, 75, 72, 3, 90, 76, 86, 72, 17, 2, 43, 68, 81, 71, 86, 3, 68, 85, 72, 3, 79, 72, 81, 87, 3, 69, 88, 87, 3, 79, 68, 90, 86, 3, 68, 85, 72, 3, 72, 81, 71, 86, 17, 2, 38, 68, 86, 72, 86, 3, 82, 81, 3, 70, 82, 88, 85, 87, 86, 3, 68, 85, 72, 3, 69, 82, 85, 85, 82, 90, 72, 71, 3, 79, 68, 81, 71, 86, 17, 2, 47, 72, 81, 74, 87, 75, 3, 79, 82, 81, 74, 3, 90, 76, 87, 75, 3, 87, 85, 72, 68, 71, 86, 3, 87, 82, 3, 85, 72, 87, 70, 75, 4, 2, 54, 87, 85, 68, 83, 86, 3, 82, 81, 3, 87, 76, 80, 72, 86, 3, 68, 81, 71, 3, 90, 68, 87, 70, 75, 3, 75, 72, 85, 72, 17, 2, 36, 85, 85, 68, 92, 86, 3, 87, 68, 81, 78, 86, 3, 69, 88, 87, 3, 68, 79, 79, 3, 68, 85, 72, 3, 80, 72, 81, 17, 2, 38, 85, 82, 86, 86, 3, 68, 79, 79, 3, 86, 88, 70, 87, 76, 82, 81, 86, 3, 86, 87, 72, 68, 79, 3, 87, 75, 72, 3, 72, 81, 71, 86, 17, 2, 38, 68, 89, 72, 3, 82, 81, 3, 80, 76, 81, 71, 86, 3, 68, 85, 72, 3, 70, 68, 74, 72, 86, 3, 82, 81, 3, 82, 69, 77, 72, 70, 87, 86, 17, 2, 53, 82, 88, 86, 72, 85, 3, 85, 82, 70, 78, 72, 87, 86, 3, 83, 82, 90, 72, 85, 86, 3, 75, 82, 79, 72, 86, 17, 2, 38, 82, 81, 73, 76, 81, 72, 3, 70, 88, 85, 86, 72, 86, 3, 87, 82, 3, 86, 87, 82, 83, 3, 82, 88, 85, 3, 90, 82, 88, 81, 71, 86, 17, 2, 58, 75, 76, 85, 79, 3, 92, 82, 88, 85, 3, 69, 82, 71, 76, 72, 86, 3, 68, 81, 71, 3, 77, 88, 80, 83, 3, 82, 81, 3, 74, 85, 82, 88, 81, 71, 86, 17, 2, 38, 85, 82, 88, 70, 75, 3, 82, 73, 3, 86, 82, 79, 71, 76, 72, 85, 86, 3, 68, 73, 87, 72, 85, 3, 78, 76, 70, 78, 86, 3, 90, 76, 87, 75, 3, 73, 79, 76, 81, 74, 86, 17, 2, 37, 79, 82, 70, 78, 3, 82, 81, 72, 3, 79, 72, 74, 3, 68, 81, 71, 3, 75, 76, 87, 3, 87, 75, 72, 3, 80, 76, 71, 71, 79, 72, 17, 2, 38, 68, 88, 70, 75, 92, 22, 3, 78, 81, 82, 90, 3, 87, 75, 72, 3, 87, 85, 76, 70, 78, 86, 3, 87, 82, 3, 78, 76, 79, 79, 17, 2, 55, 75, 85, 72, 68, 87, 72, 81, 3, 90, 72, 68, 78, 3, 82, 83, 83, 85, 72, 86, 86, 72, 71, 3, 76, 79, 79, 17, 2, 54, 88, 85, 83, 68, 86, 86, 3, 86, 70, 82, 85, 72, 86, 3, 68, 85, 72, 3, 69, 68, 71, 3, 76, 81, 3, 75, 82, 81, 82, 85, 86, 17, 2, 58, 68, 86, 75, 3, 87, 82, 3, 87, 75, 76, 81, 78, 3, 87, 75, 68, 87, 3, 69, 88, 76, 79, 71, 3, 87, 75, 72, 3, 75, 82, 80, 72, 86, 17, 2, 36, 81, 74, 72, 79, 3, 86, 76, 81, 86, 3, 69, 88, 87, 3, 70, 68, 88, 70, 75, 92, 22, 3, 75, 68, 86, 3, 73, 88, 81, 86, 17, 2, 48, 68, 78, 72, 3, 82, 81, 72, 86, 3, 87, 82, 82, 79, 86, 3, 90, 75, 72, 81, 3, 75, 68, 87, 86, 3, 68, 85, 72, 3, 73, 82, 88, 81, 71, 17, 2, 58, 82, 85, 79, 71, 86, 3, 68, 85, 72, 3, 71, 85, 68, 90, 72, 85, 86, 3, 82, 81, 3, 69, 82, 87, 87, 82, 80, 3, 81, 82, 86, 72, 86, 17, 2, 54, 76, 81, 74, 88, 79, 68, 85, 3, 88, 74, 79, 92, 3, 83, 76, 72, 70, 72, 3, 76, 86, 3, 85, 82, 86, 72, 17, 2, 58, 76, 79, 92, 3, 80, 82, 85, 72, 86, 3, 68, 85, 72, 3, 87, 72, 72, 87, 75, 3, 82, 73, 3, 86, 75, 68, 85, 78, 86, 17, 2, 54, 68, 90, 3, 90, 76, 87, 75, 3, 87, 82, 82, 87, 75, 3, 76, 86, 3, 79, 68, 90, 86, 3, 76, 81, 3, 68, 85, 87, 86, 17, 2, 36, 85, 87, 73, 88, 79, 3, 80, 72, 81, 3, 83, 82, 90, 72, 85, 3, 90, 76, 87, 75, 3, 74, 85, 76, 71, 86, 17, 2, 37, 82, 71, 76, 72, 86, 3, 86, 87, 68, 80, 83, 72, 71, 3, 68, 81, 71, 3, 90, 76, 79, 79, 86, 3, 68, 85, 72, 3, 85, 76, 71, 71, 72, 81, 17, 2, 54, 76, 74, 81, 3, 76, 81, 3, 73, 82, 85, 87, 75, 3, 90, 76, 87, 75, 3, 69, 68, 87, 87, 79, 72, 86, 3, 70, 82, 81, 84, 88, 72, 85, 72, 71, 17, 2, 55, 85, 76, 88, 80, 83, 75, 86, 3, 82, 81, 3, 70, 68, 81, 71, 79, 72, 86, 3, 90, 75, 76, 83, 3, 87, 75, 72, 3, 86, 87, 68, 81, 71, 86, 17, 2, 54, 82, 88, 83, 86, 3, 68, 85, 72, 3, 86, 82, 68, 83, 86, 3, 68, 81, 71, 3, 73, 68, 76, 87, 75, 86, 3, 81, 82, 87, 3, 70, 82, 80, 72, 17, 2, 58, 72, 3, 68, 85, 72, 3, 80, 72, 68, 87, 86, 3, 76, 81, 3, 69, 68, 79, 79, 86, 3, 68, 81, 71, 3, 85, 76, 70, 72, 3, 87, 82, 3, 70, 82, 81, 86, 87, 68, 81, 87, 86, 17, 2, 16, 16, 16, 38, 75, 72, 88, 81, 74, 3, 54, 75, 88, 81, 3, 54, 68, 81, 74, 32, 38, 68, 88, 70, 75, 92, 22, 16, 16, 16]\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.169927Z",
     "start_time": "2024-07-16T19:10:38.156108Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Testing to find new line character encoded symbol : \",char_to_int['\\n'])",
   "id": "9fd710f8c950055",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing to find new line character encoded symbol :  2\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.186036Z",
     "start_time": "2024-07-16T19:10:38.170827Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Decoded Text: \", decoded_text)",
   "id": "4dfa7642702b5921",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text:  2 ABC of H.k. and China revised vision.\n",
      "Barrels tears are wines and salts.\n",
      "With a whisk on goody tails!\n",
      "Wiggle maces to fix the heads.\n",
      "Heads in jack on boxes are ceased.\n",
      "Cry to paranoid truly bosses.\n",
      "Bosses are jokers take your boys.\n",
      "Studs are bogs with fire apples.\n",
      "True predicates worth cases.’\n",
      "Descents wash in badly bands.\n",
      "Wholly sales are smart with cats.\n",
      "Who got tenth honors in China?\n",
      "Homage grand to play and plays!\n",
      "Trim the times of hearts then cry.\n",
      "Tanks in steels but voice wail.\n",
      "Bossy dragged by tails that whisked.\n",
      "Go very timid and love the wise.\n",
      "Hands are lent but laws are ends.\n",
      "Cases on courts are borrowed lands.\n",
      "Length long with treads to retch!\n",
      "Straps on times and watch here.\n",
      "Arrays tanks but all are men.\n",
      "Cross all suctions steal the ends.\n",
      "Cave on minds are cages on objects.\n",
      "Rouser rockets powers holes.\n",
      "Confine curses to stop our wounds.\n",
      "Whirl your bodies and jump on grounds.\n",
      "Crouch of soldiers after kicks with flings.\n",
      "Block one leg and hit the middle.\n",
      "Cauchy3 know the tricks to kill.\n",
      "Threaten weak oppressed ill.\n",
      "Surpass scores are bad in honors.\n",
      "Wash to think that build the homes.\n",
      "Angel sins but cauchy3 has funs.\n",
      "Make ones tools when hats are found.\n",
      "Worlds are drawers on bottom noses.\n",
      "Singular ugly piece is rose.\n",
      "Wily mores are teeth of sharks.\n",
      "Saw with tooth is laws in arts.\n",
      "Artful men power with grids.\n",
      "Bodies stamped and wills are ridden.\n",
      "Sign in forth with battles conquered.\n",
      "Triumphs on candles whip the stands.\n",
      "Soups are soaps and faiths not come.\n",
      "We are meats in balls and rice to constants.\n",
      "---Cheung Shun Sang=Cauchy3---\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`The encoder and decoder functions have been successfully created.`\n",
    "Doing a small test to see if we can identify the encoded text."
   ],
   "id": "bd22f5460989baf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.201591Z",
     "start_time": "2024-07-16T19:10:38.187961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming the word 3 is for space as it is the most common character\n",
    "if char_to_int[' '] == 3:\n",
    "    print(\"Space is at 3\")"
   ],
   "id": "362738de497ae991",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Space is at 3\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.733333Z",
     "start_time": "2024-07-16T19:10:38.203588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizing the data using torch\n",
    "Data = [torch.tensor(text_to_tensor(text, char_to_int), dtype=torch.long).to(device) for text in data]"
   ],
   "id": "5d52d0ffc5a190a",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.749037Z",
     "start_time": "2024-07-16T19:10:40.737399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking the shape of the tensor\n",
    "print(Data[0][:10])"
   ],
   "id": "778a009548bbecbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21,  3, 36, 37, 38,  3, 82, 73,  3, 43], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Splitting the Data\n",
    "* We'll split the data into training and testing sets.\n",
    "* We'll use 80% of the data for training and 20% for testing."
   ],
   "id": "148782c89c7aa095"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.780605Z",
     "start_time": "2024-07-16T19:10:40.751039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Splitting the data into training and testing\n",
    "train_data = Data[:int(0.8*len(Data))]\n",
    "test_data = Data[int(0.8*len(Data)):]"
   ],
   "id": "5c3313269ece40ef",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.796125Z",
     "start_time": "2024-07-16T19:10:40.782648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking the length of the training and testing data\n",
    "print(len(train_data), len(test_data))"
   ],
   "id": "376a579210fe9b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5057 1265\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Loader\n",
    "### Creating Batch size and Block size\n",
    "* We'll create a batch size and block size for our model.\n",
    "* The batch size will be used to train the model.\n",
    "* The block size will be used to generate the poem."
   ],
   "id": "b0a8e7404762ab59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.812184Z",
     "start_time": "2024-07-16T19:10:40.798648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "block_size = 4\n",
    "batch_size = 8"
   ],
   "id": "ab8222cfd26ac49",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.843263Z",
     "start_time": "2024-07-16T19:10:40.814185Z"
    }
   },
   "cell_type": "code",
   "source": "torch.manual_seed(1)",
   "id": "4fa8cea2d8818c48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f6d528c270>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.859296Z",
     "start_time": "2024-07-16T19:10:40.845751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_next(data, i):\n",
    "    inputs = data[:i]\n",
    "    targets = data[i+1]\n",
    "    return inputs, targets"
   ],
   "id": "20852eb7f81032ac",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.874777Z",
     "start_time": "2024-07-16T19:10:40.861298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing the get_batch function\n",
    "for i in range(1, 10, 2):\n",
    "    inputs, targets = get_next(train_data[0], i)\n",
    "    print(inputs, targets)"
   ],
   "id": "b15671c3be2993f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21], device='cuda:0') tensor(36, device='cuda:0')\n",
      "tensor([21,  3, 36], device='cuda:0') tensor(38, device='cuda:0')\n",
      "tensor([21,  3, 36, 37, 38], device='cuda:0') tensor(82, device='cuda:0')\n",
      "tensor([21,  3, 36, 37, 38,  3, 82], device='cuda:0') tensor(3, device='cuda:0')\n",
      "tensor([21,  3, 36, 37, 38,  3, 82, 73,  3], device='cuda:0') tensor(17, device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.890294Z",
     "start_time": "2024-07-16T19:10:40.875786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = pad_sequence([data[i][:block_size] for i in ix], batch_first=True, padding_value=0)\n",
    "    y = pad_sequence([data[i][1:block_size+1] for i in ix], batch_first=True, padding_value=0)\n",
    "    return x, y"
   ],
   "id": "b7461068daf84561",
   "outputs": [],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.905849Z",
     "start_time": "2024-07-16T19:10:40.892198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing the get_batch function\n",
    "x, y = get_batch('train')"
   ],
   "id": "32fe32828cc09cec",
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.921002Z",
     "start_time": "2024-07-16T19:10:40.910788Z"
    }
   },
   "cell_type": "code",
   "source": "print(x.shape, y.shape)",
   "id": "609923db019d06b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4]) torch.Size([8, 4])\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.936286Z",
     "start_time": "2024-07-16T19:10:40.922377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = x[b, :t+1]\n",
    "        target = y[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ],
   "id": "fb99bf54cd4edb13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [55] the target: 75\n",
      "when input is [55, 75] the target: 72\n",
      "when input is [55, 75, 72] the target: 85\n",
      "when input is [55, 75, 72, 85] the target: 72\n",
      "when input is [37] the target: 85\n",
      "when input is [37, 85] the target: 76\n",
      "when input is [37, 85, 76] the target: 74\n",
      "when input is [37, 85, 76, 74] the target: 75\n",
      "when input is [40] the target: 3\n",
      "when input is [40, 3] the target: 89\n",
      "when input is [40, 3, 89] the target: 10\n",
      "when input is [40, 3, 89, 10] the target: 85\n",
      "when input is [43] the target: 82\n",
      "when input is [43, 82] the target: 90\n",
      "when input is [43, 82, 90] the target: 3\n",
      "when input is [43, 82, 90, 3] the target: 75\n",
      "when input is [53] the target: 88\n",
      "when input is [53, 88] the target: 87\n",
      "when input is [53, 88, 87] the target: 75\n",
      "when input is [53, 88, 87, 75] the target: 76\n",
      "when input is [49] the target: 82\n",
      "when input is [49, 82] the target: 3\n",
      "when input is [49, 82, 3] the target: 71\n",
      "when input is [49, 82, 3, 71] the target: 82\n",
      "when input is [55] the target: 75\n",
      "when input is [55, 75] the target: 72\n",
      "when input is [55, 75, 72] the target: 85\n",
      "when input is [55, 75, 72, 85] the target: 72\n",
      "when input is [36] the target: 79\n",
      "when input is [36, 79] the target: 87\n",
      "when input is [36, 79, 87] the target: 72\n",
      "when input is [36, 79, 87, 72] the target: 85\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:40.952146Z",
     "start_time": "2024-07-16T19:10:40.937763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Checking the shape of the tensor\n",
    "print(x.shape, y.shape)"
   ],
   "id": "3d5cbe8eca8c83f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4]) torch.Size([8, 4])\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "* We'll now create the model.\n",
    "* Using the Bigram Language Model, we'll create the model.\n",
    "* The model will be trained on the dataset."
   ],
   "id": "43d048ca2a77b7fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:14:01.723009Z",
     "start_time": "2024-07-16T19:14:01.697568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "    The Bigram Language Model is a simple model that uses the previous word to predict the next word.\n",
    "    It is a simple model that can be used to generate text.\n",
    "    It works on a statistical principle that the probability of a word depends on the previous word.\n",
    "'''\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        logits = self.token_embedding(x)\n",
    "\n",
    "        \n",
    "        if targets is None:\n",
    "            loss_Value = None\n",
    "        else:\n",
    "            B , T , C = logits.shape\n",
    "    \n",
    "            Product = B*T\n",
    "    \n",
    "            logits = logits.view(Product,C)\n",
    "    \n",
    "            targets = targets.view(-1)\n",
    "             \n",
    "            # print(\"Logits shape: \", logits.shape)\n",
    "            # print(\"Targets shape: \", targets.shape)\n",
    "            \n",
    "            loss_Value = F.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits, loss_Value\n",
    "    \n",
    "    def generate(self, idx, n):\n",
    "        # Generate n tokens\n",
    "        for _ in range(n):\n",
    "            # get predication for the next token\n",
    "            '''\n",
    "                We are getting the prediction for the next token.\n",
    "                This is done by passing the input through the model.\n",
    "                Ignoring the loss as we are not training the model.\n",
    "            '''\n",
    "            logit, _ = self(idx)\n",
    "            \n",
    "            # focusing on the last token\n",
    "            '''\n",
    "                We are focusing on the last token as it tells us the probability of the next token to be generated.\n",
    "            '''\n",
    "            Temp = logit[:, -1, :]\n",
    "            \n",
    "            \n",
    "            # getting the token with the probability using softmax\n",
    "            '''\n",
    "                There are multiple possible tokens that can be generated. So we generative them into a probability distribution.\n",
    "                This is done using the softmax function. The softmax function converts the logits into a probability distribution.\n",
    "                This probability distribution gives each token a probability of being generated out of 100%.\n",
    "            '''\n",
    "            probability = F.softmax(Temp,dim=-1)\n",
    "            \n",
    "            \n",
    "            # sample from the probability distribution\n",
    "            '''\n",
    "                We are sampling from the probability distribution to get the next token.\n",
    "                This is done using the multinomial function.\n",
    "                The multinomial function generates a random sample from the probability distribution.\n",
    "                By passing '1' as the second argument, we are generating a single token.\n",
    "            '''\n",
    "            next_token = torch.multinomial(probability,1)\n",
    "            \n",
    "            # add the token to the input\n",
    "            '''\n",
    "                We are adding the token to the input.\n",
    "                This is done by concatenating the token to the input.\n",
    "                This is done so that we can predict the next token using the previously added one as well.\n",
    "            '''\n",
    "            idx = torch.cat([idx,next_token],dim=-1)\n",
    "        return idx"
   ],
   "id": "ca75d04a268f9da0",
   "outputs": [],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:14:07.237993Z",
     "start_time": "2024-07-16T19:14:07.211548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing the model\n",
    "model = BigramLanguageModel(vocabulary_size).to(device)\n",
    "\n",
    "output, loss = model(x,y)"
   ],
   "id": "7a41b932c8e8ca2a",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:12:32.727278Z",
     "start_time": "2024-07-16T19:12:32.716560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Loss = \", loss)\n",
    "print(\"Output = \", output)"
   ],
   "id": "95397f2c51322a10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  tensor(7.3476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Output =  tensor([[ 1.6153,  2.1874, -0.6893,  ..., -0.5108,  0.2765,  1.9100],\n",
      "        [ 0.1448, -0.7236,  0.4016,  ...,  1.2558, -1.0230,  0.9542],\n",
      "        [-0.0647, -0.4672, -0.8930,  ..., -2.3695,  1.7863, -0.4483],\n",
      "        ...,\n",
      "        [-0.6421, -0.9916, -0.4152,  ...,  1.0587,  0.8189, -0.4496],\n",
      "        [-1.0701, -0.4173, -0.7640,  ..., -0.6509,  0.4878,  0.1484],\n",
      "        [-0.0647, -0.4672, -0.8930,  ..., -2.3695,  1.7863, -0.4483]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:19:34.527879Z",
     "start_time": "2024-07-16T19:19:34.512828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# using the value '2' as it is the value for new like character as previously found\n",
    "idx =  torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "max_length = 100"
   ],
   "id": "a522144290a03fc1",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:19:34.856035Z",
     "start_time": "2024-07-16T19:19:34.843159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#making the complete value 2 as its the value for new line character\n",
    "idx.fill_(2)\n",
    "print(idx)"
   ],
   "id": "c45792f8c2755cfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:19:35.433118Z",
     "start_time": "2024-07-16T19:19:35.342942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BigramLanguageModel(vocabulary_size).to(device)\n",
    "encoded_answer = model.generate(idx, max_length)[0].tolist()"
   ],
   "id": "ea062e9c01d49f61",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:19:35.748022Z",
     "start_time": "2024-07-16T19:19:35.738403Z"
    }
   },
   "cell_type": "code",
   "source": "print(tensor_to_text(encoded_answer, int_to_char))",
   "id": "fa4d36a1ec36e52e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "在#Éφ在০<缠此ज़席৬开七下至玉प东可皆[য়载更प别城भω拨沙净ی嘈Ó音য়pþ乐L行承镜世九空Èस结梨纽震*喜f目ट娥倾Dế把亚调￼检不8ợ祝Ø颤机य试ếAλα迟泽Ø​◆皆世繁然飞़空：ω|浪đớC\n"
     ]
    }
   ],
   "execution_count": 191
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Optimizing the model",
   "id": "be1e6ecdce80f657"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "    This will update the parameters by taking the gradient decent.\n",
    "'''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "fd9cb2aefc7696f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
